
![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211206161018.png)

### 3.1 TCP 头格式

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211206161759.png)

**序列号**：在建⽴连接时由计算机⽣成的随机数作为其初始值，通过 SYN 包传给接收端主机，每发送⼀次数据，就「累加」⼀次该「数据字节数」的⼤⼩。**⽤来解决⽹络包乱序问题**。


**确认应答号**：指下⼀次「期望」收到的数据的序列号，发送端收到这个确认应答以后可以认为在这个序号以前的数据都已经被正常接收。**⽤来解决不丢包的问题**。


**控制位**：
- ACK：该位为 1 时，「确认应答」的字段变为有效，TCP 规定除了最初建⽴连接时的 SYN 包之外该位必
须设置为 1 。

- RST：该位为 1 时，表示 TCP 连接中出现异常必须强制断开连接。

- SYN：该位为 1 时，表示希望建⽴连接，并在其「序列号」的字段进⾏序列号初始值的设定。

- FIN：该位为 1 时，表示今后不会再有数据发送，希望断开连接。当通信结束希望断开连接时，通信双⽅的
主机之间就可以相互交换 FIN 位为 1 的 TCP 段。



### 3.2 为什么需要 TCP 协议？ TCP ⼯作在哪⼀层？
IP 层是「不可靠」的，它不保证⽹络包的交付、不保证⽹络包的按序交付、也不保证⽹络包中的数据的完整性。


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211207193322.png)


如果需要保障⽹络数据包的可靠性，那么就需要由上层（传输层）的 TCP 协议来负责。

因为 TCP 是⼀个⼯作在**传输层**的可靠数据传输的服务，它能确保接收端接收的⽹络包是 **⽆损坏、⽆间隔、⾮冗余和按序的**。

### 3.3 什么是TCP
TCP 是 **⾯向连接的**、**可靠的**、**基于字节流**的传输层通信协议。

- ⾯向连接：⼀定是「⼀对⼀」才能连接，不能像 UDP 协议可以⼀个主机同时向多个主机发送消息，也就是⼀对多是⽆法做到的；

- 可靠的：⽆论的⽹络链路中出现了怎样的链路变化，TCP 都可以保证⼀个报⽂⼀定能够到达接收端；


- 字节流：消息是「**没有边界**」的，所以⽆论我们消息有多⼤都可以进⾏传输。并且消息是「有序的」，当「前⼀个」消息没有收到的时候，即使它先收到了后⾯的字节，那么也不能扔给应⽤层去处理，同时对「 
复」的报⽂会⾃动丢弃。


### 3.4 什么是 TCP 连接？
简单来说就是，⽤于保证可靠性和流量控制维护的某些状态信息，这些信息的组合，包括Socket、序列号和窗⼝⼤⼩称为连接。

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211207201818.png)



所以我们可以知道，建⽴⼀个 TCP 连接是需要客户端与服务器端达成上述三个信息的共识。

- Socket：由 IP 地址和端⼝号组成
- 序列号：⽤来解决乱序问题等
- 窗⼝⼤⼩：⽤来做流量控制


### 3.5 如何唯⼀确定⼀个 TCP 连接呢？
TCP 四元组可以唯⼀的确定⼀个连接，四元组包括如下：
![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211207203647.png)

源地址和⽬的地址的字段（32位）是在 IP 头部中，作⽤是通过 IP 协议发送报⽂给对⽅主机。

源端⼝和⽬的端⼝的字段（16位）是在 TCP 头部中，作⽤是告诉 TCP 协议应该把报⽂发给哪个进程。


### 3.6 有⼀个 IP 的服务器监听了⼀个端⼝，它的 TCP 的最⼤连接数是多少？
服务器通常固定在某个本地端⼝上监听，等待客户端的连接请求。

因此，客户端 IP 和 端⼝是可变的，其理论值计算公式如下:
![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211207205930.png)


对 IPv4，客户端的 IP 数最多为 2 的 32 次⽅，客户端的端⼝数最多为 2 的 16 次⽅，也就是服务端单机最⼤ TCP 连接数，约为 2 的 48 次⽅。


当然，服务端最⼤并发 TCP 连接数远不能达到理论上限。

- ⾸先主要是⽂件描述符限制，Socket 都是⽂件，所以⾸先要通过 ulimit 配置⽂件描述符的数⽬；
- 另⼀个是内存限制，每个 TCP 连接都要占⽤⼀定内存，操作系统的内存是有限的。

### 3.7 UDP 和 TCP 有什么区别呢？分别的应⽤场景是？
UDP 不提供复杂的控制机制，利⽤ IP 提供⾯向「⽆连接」的通信服务。

UDP 协议真的⾮常简单，头部只有 8 个字节（ 64 位），UDP 的头部格式如下：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211207212144.png)

- ⽬标和源端⼝：主要是告诉 UDP 协议应该把报⽂发给哪个进程。
- 包⻓度：该字段保存了 UDP ⾸部的⻓度跟数据的⻓度之和。
- 校验和：校验和是为了提供可靠的 UDP ⾸部和数据⽽设计。


#### TCP 和 UDP 区别：
##### 1. 连接
- TCP 是⾯向连接的传输层协议，传输数据前先要建⽴连接。
- UDP 是不需要连接，即刻传输数据。


##### 2. 服务对象
- TCP 是⼀对⼀的两点服务，即⼀条连接只有两个端点。
- UDP ⽀持⼀对⼀、⼀对多、多对多的交互通信


##### 3. 可靠性
- TCP 是可靠交付数据的，数据可以⽆差错、不丢失、不重复、按序到达。
- UDP 是尽最⼤努⼒交付，不保证可靠交付数据。


##### 4. 拥塞控制、流量控制
- TCP 有拥塞控制和流量控制机制，保证数据传输的安全性。
- UDP 则没有，即使⽹络⾮常拥堵了，也不会影响 UDP 的发送速率。

##### 5. ⾸部开销
- TCP ⾸部⻓度较⻓，会有⼀定的开销，⾸部在没有使⽤「选项」字段时是 20 个字节，如果使⽤了「选项」字段则会变⻓的。
- UDP ⾸部只有 8 个字节，并且是固定不变的，开销较⼩。




##### 6. 传输⽅式
- TCP 是流式传输，没有边界，但保证顺序和可靠。
- UDP 是⼀个包⼀个包的发送，是有边界的，但可能会丢包和乱序。



##### 7. 分⽚不同
- TCP 的数据⼤⼩如果⼤于 MSS ⼤⼩，则会在传输层进⾏分⽚，⽬标主机收到后，也同样在传输层组装 TCP
数据包，如果中途丢失了⼀个分⽚，只需要传输丢失的这个分⽚。

- UDP 的数据⼤⼩如果⼤于 MTU ⼤⼩，则会在 IP 层进⾏分⽚，⽬标主机收到后，在 IP 层组装完数据，接着再传给传输层，但是如果中途丢了⼀个分⽚，在实现可靠传输的 UDP 时则就需要重传所有的数据包，这样传输效率⾮常差，所以通常 UDP 的报⽂应该⼩于 MTU。

### TCP 和 UDP 应⽤场景：
由于 TCP 是⾯向连接，能保证数据的可靠性交付，因此经常⽤于：

- FTP ⽂件传输
- HTTP / HTTPS

由于 UDP ⾯向⽆连接，它可以随时发送数据，再加上UDP本身的处理既简单⼜⾼效，因此经常⽤于：

- 包总量较少的通信，如 DNS 、 SNMP 等
- 视频、⾳频等多媒体通信
- ⼴播通信


### 3.8 为什么 UDP 头部没有「⾸部⻓度」字段，⽽ TCP 头部有「⾸部⻓度」字段呢？

原因是 TCP 有可变⻓的「选项」字段，⽽ UDP 头部⻓度则是不会变化的，⽆需多⼀个字段去记录 UDP 的⾸部⻓度。


###  3.9 为什么 UDP 头部有「包⻓度」字段，⽽ TCP 头部则没有「包⻓度」字段呢？
先说说 TCP 是如何计算负载数据⻓度：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211207220844.png)

其中 IP 总⻓度 和 IP ⾸部⻓度，在 IP ⾸部格式是已知的。TCP ⾸部⻓度，则是在 TCP ⾸部格式已知的，所以就可以求得 TCP 数据的⻓度。


⼤家这时就奇怪了问：“ UDP 也是基于 IP 层的呀，那 UDP 的数据⻓度也可以通过这个公式计算呀？ 为何还要有「包⻓度」呢？”

这么⼀问，确实感觉 UDP 「包⻓度」是冗余的。

因为为了⽹络设备硬件设计和处理⽅便，⾸部⻓度需要是 4 字节的整数倍。


### 3.10 TCP 三次握⼿过程和状态变迁

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211207223731.png)


-  ⼀开始，客户端和服务端都处于 CLOSED 状态。先是服务端主动监听某个端⼝，处于 LISTEN 状态

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211207224157.png)


- 客户端会随机初始化序号（ **client_isn** ），将此序号置于 TCP ⾸部的「序号」字段中，同时把 **SYN** 标志位置为 **1** ，表示 **SYN** 报⽂。接着把第⼀个 SYN 报⽂发送给服务端，表示向服务端发起连接，该报⽂不包含应⽤层数据，之后客户端处于 **SYN-SENT** 状态。

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211208100858.png)

- 服务端收到客户端的 SYN 报⽂后，⾸先服务端也随机初始化⾃⼰的序号（ server_isn ），将此序号填⼊TCP ⾸部的「序号」字段中,其次把  client_isn + 1 填⼊TCP ⾸部的「确认应答号」字段中。
接着把 SYN和 ACK 标志位置为 1 。最后把该报⽂发给客户端，该报⽂也不包含应⽤层数据，之后服务端处于 SYN-RCVD 状态。

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211208102524.png)


- 客户端收到服务端报⽂后，还要向服务端回应最后⼀个应答报⽂，⾸先该应答报⽂ TCP ⾸部 ACK 标志位
置为 1 ，其次「确认应答号」字段填⼊ server_isn + 1 ，最后把报⽂发送给服务端，这次报⽂可以携带客户到服务器的数据，之后客户端处于 ESTABLISHED 状态。


- 服务器收到客户端的应答报⽂后，也进⼊ ESTABLISHED 状态。

从上⾯的过程可以发现**第三次握⼿是可以携带数据的，前两次握⼿是不可以携带数据的**，这也是⾯试常问的题。

⼀旦完成三次握⼿，双⽅都处于 ESTABLISHED 状态，此时连接就已建⽴完成，客户端和服务端就可以相互发送数据了。



### 3.11 如何在 Linux 系统中查看 TCP 状态？
TCP 的连接状态查看，在 Linux 可以通过 netstat -napt 命令查看。

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211208104358.png)


### 3.11 为什么是三次握⼿？不是两次、四次？
在前⾯我们知道了什么是 TCP 连接：

- ⽤于保证可靠性和流量控制维护的某些状态信息，这些信息的组合，包括Socket、序列号和窗⼝⼤⼩称为连接。


所以，重要的是为什么三次握⼿才可以初始化Socket、序列号和窗⼝⼤⼩并建⽴ TCP 连接。

接下来以三个⽅⾯分析三次握⼿的原因：

- 三次握⼿才可以阻⽌重复历史连接的初始化（主要原因）
- 三次握⼿才可以同步双⽅的初始序列号
- 三次握⼿才可以避免资源浪费

#### 原因⼀：避免历史连接

三次握⼿的⾸要原因是为了防⽌旧的重复连接初始化造成混乱。

⽹络环境是错综复杂的，往往并不是如我们期望的⼀样，先发送的数据包，就先到达⽬标主机，反⽽它很骚，可能会由于⽹络拥堵等乱七⼋糟的原因，会使得旧的数据包，先到达⽬标主机，那么这种情况下 TCP 三次握⼿是如何避免的呢？

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211208111957.png)


客户端连续发送多次 SYN 建⽴连接的报⽂，在⽹络拥堵情况下：

- ⼀个「旧 SYN 报⽂」⽐「最新的 SYN 」 报⽂早到达了服务端；
- 那么此时服务端就会回⼀个 SYN + ACK 报⽂给客户端；
- 客户端收到后可以根据⾃身的上下⽂，判断这是⼀个历史连接（序列号过期或超时），那么客户端就会发送RST 报⽂给服务端，表示中⽌这⼀次连接。

如果是两次握⼿连接，就不能判断当前连接是否是历史连接，三次握⼿则可以在客户端（发送⽅）准备发送第三次报⽂时，客户端因有⾜够的上下⽂来判断当前连接是否是历史连接：

- 如果是历史连接（序列号过期或超时），则第三次握⼿发送的报⽂是 RST 报⽂，以此中⽌历史连接；

- 如果不是历史连接，则第三次发送的报⽂是 ACK 报⽂，通信双⽅就会成功建⽴连接；

所以，TCP 使⽤三次握⼿建⽴连接的最主要原因是**防⽌历史连接初始化了连接**。


#### 原因⼆：同步双⽅初始序列号
TCP 协议的通信双⽅， 都必须维护⼀个「序列号」， 序列号是可靠传输的⼀个关键因素，它的作⽤：
- 接收⽅可以去除重复的数据；
- 接收⽅可以根据数据包的序列号按序接收；
- 可以标识发送出去的数据包中， 哪些是已经被对⽅收到的；

可⻅，序列号在 TCP 连接中占据着⾮常重要的作⽤，所以当客户端发送携带「初始序列号」的 SYN 报⽂的时候，需要服务端回⼀个 ACK 应答报⽂，表示客户端的 SYN 报⽂已被服务端成功接收，那当服务端发送「初始序列号」给客户端的时候，依然也要得到客户端的应答回应，这样⼀来⼀回，才能确保双⽅的初始序列号能被可靠的同步。


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211208124703.png)


四次握⼿其实也能够可靠的同步双⽅的初始化序号，但由于第⼆步和第三步可以优化成⼀步，所以就成了「三次握⼿」。

⽽两次握⼿只保证了⼀⽅的初始序列号能被对⽅成功接收，没办法保证双⽅的初始序列号都能被确认接收。


#### 原因三：避免资源浪费
如果只有「两次握⼿」，当客户端的 SYN 请求连接在⽹络中阻塞，客户端没有接收到 ACK 报⽂，就会重新发送 SYN ，由于没有第三次握⼿，服务器不清楚客户端是否收到了⾃⼰发送的建⽴连接的 ACK 确认信号，所以每收到⼀个 SYN 就只能先主动建⽴⼀个连接，这会造成什么情况呢？

如果客户端的 SYN 阻塞了， 重复发送多次 SYN 报⽂，那么服务器在收到请求后就会建⽴多个冗余的⽆效链
接，造成不必要的资源浪费。

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211208125601.png)

即两次握⼿会造成消息滞留情况下，服务器重复接受⽆⽤的连接请求 SYN 报⽂，⽽造成重复分配资源。


### 3.12 为什么是三次握⼿？不是两次、四次？
TCP 建⽴连接时，通过三次握⼿能**防⽌历史连接的建⽴**，能**减少双⽅不必要的资源开销**，能**帮助双⽅同步初始化序列号**。序列号能够保证数据包不重复、不丢弃和按序传输。

不使⽤「两次握⼿」和「四次握⼿」的原因：

- 「两次握⼿」：⽆法防⽌历史连接的建⽴，会造成双⽅资源的浪费，也⽆法可靠的同步双⽅序列号；
- 「四次握⼿」：三次握⼿就已经理论上最少可靠连接建⽴，所以不需要使⽤更多的通信次数。


### 3.13 为什么客户端和服务端的初始序列号 ISN 是不相同的？

如果⼀个已经失效的连接被重⽤了，但是该旧连接的历史报⽂还残留在⽹络中，如果序列号相同，那么就⽆法分辨出该报⽂是不是历史报⽂，如果历史报⽂被新的连接接收了，则会产⽣数据错乱。

所以，每次建⽴连接前重新初始化⼀个序列号主要是为了通信双⽅能够根据序号将不属于本连接的报⽂段丢弃。

另⼀⽅⾯是为了安全性，防⽌⿊客伪造的相同序列号的 TCP 报⽂被对⽅接收。


### 3.14 初始序列号 ISN 是如何随机产⽣的？
起始 ISN 是基于时钟的，每 4 毫秒 + 1，转⼀圈要 4.55 个⼩时。

RFC1948 中提出了⼀个较好的初始化序列号 ISN 随机⽣成算法。


- ISN = M + F (localhost, localport, remotehost, remoteport)

M 是⼀个计时器，这个计时器每隔 4 毫秒加 1。

F 是⼀个 Hash 算法，根据源 IP、⽬的 IP、源端⼝、⽬的端⼝⽣成⼀个随机数值。要保证 Hash 算法不能
被外部轻易推算得出，⽤ MD5 算法是⼀个⽐较好的选择。

### 3.15 既然 IP 层会分⽚，为什么 TCP 层还需要 MSS 呢？
我们先来认识下 MTU 和 MSS
![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211208192751.png)

- MTU ：⼀个⽹络包的最⼤⻓度，以太⽹中⼀般为 1500 字节；
- MSS ：除去 IP 和 TCP 头部之后，⼀个⽹络包所能容纳的 TCP 数据的最⼤⻓度；

如果在 TCP 的整个报⽂（头部 + 数据）交给 IP 层进⾏分⽚，会有什么异常呢？

当 IP 层有⼀个超过 MTU ⼤⼩的数据（TCP 头部 + TCP 数据）要发送，那么 IP 层就要进⾏分⽚，把数据分⽚成若⼲⽚，保证每⼀个分⽚都⼩于 MTU。把⼀份 IP 数据报进⾏分⽚以后，由⽬标主机的 IP 层来进⾏ 重新组装后，再交给上⼀层 TCP 传输层。

这看起来井然有序，但这存在隐患的，那么当如果⼀个 IP 分⽚丢失，整个 IP 报⽂的所有分⽚都得重传。

因为 IP 层本身没有超时重传机制，它由传输层的 TCP 来负责超时和重传。

当接收⽅发现 TCP 报⽂（头部 + 数据）的某⼀⽚丢失后，则不会响应 ACK 给对⽅，那么发送⽅的 TCP 在超时后，就会重发「整个 TCP 报⽂（头部 + 数据）」。

因此，可以得知由 IP 层进⾏分⽚传输，是⾮常没有效率的。


所以，为了达到最佳的传输效能 TCP 协议在建⽴连接的时候通常要协商双⽅的 MSS 值，当 TCP 层发现数据超过MSS 时，则就先会进⾏分⽚，当然由它形成的 IP 包的⻓度也就不会⼤于 MTU ，⾃然也就不⽤ IP 分⽚了。


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211208221659.png)


经过 TCP 层分⽚后，如果⼀个 TCP 分⽚丢失后，**进⾏重发时也是以 MSS 为单位**，⽽不⽤重传所有的分⽚，⼤⼤增加了重传的效率。



### 3.16 什么是 SYN 攻击？如何避免 SYN 攻击？

#### SYN 攻击
我们都知道 TCP 连接建⽴是需要三次握⼿，假设攻击者短时间伪造不同 IP 地址的 SYN 报⽂，服务端每接收到⼀个 SYN 报⽂，就进⼊ SYN_RCVD 状态，但服务端发送出去的 ACK + SYN 报⽂，⽆法得到未知 IP 主机的ACK 应答，久⽽久之就会占满服务端的 SYN 接收队列（未连接队列），使得服务器不能为正常⽤户服务。



#### 避免 SYN 攻击⽅式⼀
其中⼀种解决⽅式是通过修改 Linux 内核参数，控制队列⼤⼩和当队列满时应做什么处理。

- 当⽹卡接收数据包的速度⼤于内核处理的速度时，会有⼀个队列保存这些数据包。控制该队列的最⼤值如下参数：
net.core.netdev_max_backlog

- SYN_RCVD 状态连接的最⼤个数：
net.ipv4.tcp_max_syn_backlog

- 超出处理能力时，对新的 SYN 直接回复 RST，丢弃连接：
net.ipv4.tcp_abort_on_overflow




#### 避免 SYN 攻击⽅式二
我们先来看下 Linux 内核的 SYN （未完成连接建⽴）队列与 Accpet （已完成连接建⽴）队列是如何⼯作的？

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211209122108.png)


##### 正常流程：
- 当服务端接收到客户端的 SYN 报⽂时，会将其加⼊到内核的「 SYN 队列」；
- 接着发送 SYN + ACK 给客户端，等待客户端回应 ACK 报⽂；
- 服务端接收到 ACK 报⽂后，从「 SYN 队列」移除放⼊到「 Accept 队列」；
- 应⽤通过调⽤ accpet() socket 接⼝，从「 Accept 队列」取出连接。


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211209122448.png)


##### 应⽤程序过慢：
- 如果应⽤程序过慢时，就会导致「 Accept 队列」被占满。

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211209122909.png)


##### 受到 SYN 攻击：
- 如果不断受到 SYN 攻击，就会导致「 SYN 队列」被占满。

tcp_syncookies 的⽅式可以应对 SYN 攻击的⽅法：
net.ipv4.tcp_syncookies = 1

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211209123310.png)

- 当 「 SYN 队列」满之后，后续服务器收到 SYN 包，不进⼊「 SYN 队列」；

- 计算出⼀个 cookie 值，再以 SYN + ACK 中的「序列号」返回客户端，

- 服务端接收到客户端的应答报⽂时，服务器会检查这个 ACK 包的合法性。如果合法，直接放⼊到「 Accept队列」。


- 最后应⽤通过调⽤ accpet() socket 接⼝，从「 Accept 队列」取出的连接。


### 3.17 TCP 四次挥⼿过程和状态变迁

双⽅都可以主动断开连接，断开连接后主机中的「资源」将被释放。
![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211209125340.png)


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211209125507.png)

这⾥⼀点需要注意是：主动关闭连接的，才有 TIME_WAIT 状态。



### 3.18 为什么挥⼿需要四次？

- 关闭连接时，客户端向服务端发送 FIN 时，仅仅表示客户端不再发送数据了但是还能接收数据。

- 服务器收到客户端的 FIN 报⽂时，先回⼀个 ACK 应答报⽂，⽽服务端可能还有数据需要处理和发送，等
服务端不再发送数据时，才发送 FIN 报⽂给客户端来表示同意现在关闭连接。

从上⾯过程可知，**服务端通常需要等待完成数据的发送和处理**，所以服务端的 ACK 和 FIN ⼀般都会分开发送，从⽽⽐三次握⼿导致多了⼀次。


### 3.19 为什么 TIME_WAIT 等待的时间是 2MSL？

MSL 是 Maximum Segment Lifetime，**报⽂最⼤⽣存时间**，它是任何报⽂在⽹络上存在的最⻓时间，超过这个时间报⽂将被丢弃。

因为 TCP 报⽂基于是 IP 协议的，⽽ IP 头中有⼀个 **TTL** 字段，是 IP 数据报可以经过的最⼤路
由数，每经过⼀个处理他的路由器此值就减 1，当此值为 0 则数据报将被丢弃，同时发送 ICMP 报⽂通知源主机。

MSL 与 TTL 的区别： MSL 的单位是时间，⽽ TTL 是经过路由跳数。所以 MSL 应该要⼤于等于 TTL 消耗为 0 的时间，以确保报⽂已被⾃然消亡。

TIME_WAIT 等待 2 倍的 MSL，⽐较合理的解释是： ⽹络中可能存在来⾃发送⽅的数据包，当这些发送⽅的数据包被接收⽅处理后⼜会向对⽅发送响应，所以 **⼀去⼀回需要等待 2 倍的时间**。


⽐如如果被动关闭⽅没有收到断开连接的最后的 ACK 报⽂，就会触发超时 发 Fin 报⽂，另⼀⽅接收到 FIN 后，会重发 ACK 给被动关闭⽅， ⼀来⼀去正好 2 个 MSL。

2MSL 的时间是从客户端接收到 FIN 后发送 ACK 开始计时的。如果在 TIME-WAIT 时间内，因为客户端的 ACK没有传输到服务端，客户端⼜接收到了服务端重发的 FIN 报⽂，那么 2MSL 时间将重新计时。

在 Linux 系统⾥ 2MSL 默认是 60 秒，那么⼀个 MSL 也就是 30 秒。Linux 系统停留在 TIME_WAIT 的时
间为固定的 60 秒。


其定义在 Linux 内核代码⾥的名称为 TCP_TIMEWAIT_LEN：

- #define TCP_TIMEWAIT_LEN (60*HZ) /* how long to wait to destroy TIME-WAIT state, about 60 seconds */

如果要修改 TIME_WAIT 的时间⻓度，只能修改 Linux 内核代码⾥ TCP_TIMEWAIT_LEN 的值，并 新编译 Linux内核。



### 3.20 为什么需要 TIME_WAIT 状态？
主动发起关闭连接的⼀⽅，才会有 TIME-WAIT 状态。

需要 TIME-WAIT 状态，主要是两个原因：
- 防⽌具有相同「四元组」的「旧」数据包被收到；
- 保证「被动关闭连接」的⼀⽅能被正确的关闭，即保证最后的 ACK 能让被动关闭⽅接收，从⽽帮助其正常关闭；

#### 原因⼀：防⽌旧连接的数据包
假设 TIME-WAIT 没有等待时间或时间过短，被延迟的数据包抵达后会发⽣什么呢？

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211209133055.png)

- 如上图⻩⾊框框服务端在关闭连接之前发送的 SEQ = 301 报⽂，被⽹络延迟了。

- 这时有相同端⼝的 TCP 连接被复⽤后，被延迟的 SEQ = 301 抵达了客户端，那么客户端是有可能正常接收这个过期的报⽂，这就会产⽣数据错乱等严 的问题。


所以，TCP 就设计出了这么⼀个机制，经过 2MSL 这个时间，⾜以让两个⽅向上的数据包都被丢弃，使得原来连接的数据包在⽹络中都⾃然消失，再出现的数据包⼀定都是新建⽴连接所产⽣的。

#### 原因⼆：保证连接正确关闭
TIME-WAIT 作⽤是等待⾜够的时间以**确保最后的 ACK 能让被动关闭⽅接收，从⽽帮助其正常关闭**。

假设 TIME-WAIT 没有等待时间或时间过短，断开连接会造成什么问题呢？

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211209134441.png)

- 如上图红⾊框框客户端四次挥⼿的最后⼀个 ACK 报⽂如果在⽹络中被丢失了，此时如果客户端 TIME-
WAIT 过短或没有，则就直接进⼊了 CLOSED 状态了，那么服务端则会⼀直处在 LASE_ACK 状态。

- 当客户端发起建⽴连接的 SYN 请求报⽂后，服务端会发送 RST 报⽂给客户端，连接建⽴的过程就会被
终⽌。

如果 TIME-WAIT 等待⾜够⻓的情况就会遇到两种情况：
- 服务端正常收到四次挥⼿的最后⼀个 ACK 报⽂，则服务端正常关闭连接。

- 服务端没有收到四次挥⼿的最后⼀个 ACK 报⽂时，则会 发 FIN 关闭连接报⽂并等待新的 ACK 报
⽂。

所以客户端在 TIME-WAIT 状态等待 2MSL 时间后，就可以保证双⽅的连接都可以正常的关闭。


### 3.21 TIME_WAIT 过多有什么危害？

如果服务器有处于 TIME-WAIT 状态的 TCP，则说明是由服务器⽅主动发起的断开请求。

过多的 TIME-WAIT 状态主要的危害有两种：

- 第⼀是内存资源占⽤；
- 第⼆是对端⼝资源的占⽤，⼀个 TCP 连接⾄少消耗⼀个本地端⼝；

第⼆个危害是会造成严 的后果的，要知道，端⼝资源也是有限的，⼀般可以开启的端⼝为 32768～61000 ，也可以通过如下参数设置指定net.ipv4.ip_local_port_range

**如果发起连接⼀⽅的 TIME_WAIT 状态过多，占满了所有端⼝资源，则会导致⽆法创建新连接。**


客户端受端⼝资源限制：
- 客户端TIME_WAIT过多，就会导致端⼝资源被占⽤，因为端⼝就65536个，被占满就会导致⽆法创建新的连
接。

服务端受系统资源限制：
- 由于⼀个四元组表示 TCP 连接，理论上服务端可以建⽴很多连接，服务端确实只监听⼀个端⼝ 但是会把连接扔给处理线程，所以理论上监听的端⼝可以继续监听。但是线程池处理不了那么多⼀直不断的连接了。所以当服务端出现⼤  TIME_WAIT 时，系统资源被占满时，会导致处理不过来新的连接。

### 3.22 如何优化 TIME_WAIT？
这⾥给出优化 TIME-WAIT 的⼏个⽅式，都是有利有弊：

- 方式一 ：打开 net.ipv4.tcp_tw_reuse 和 net.ipv4.tcp_timestamps 选项；

如下的 Linux 内核参数开启后，则可以**复⽤处于 TIME_WAIT 的 socket 为新的连接所⽤**。

有⼀点需要注意的是，**tcp_tw_reuse 功能只能⽤客户端（连接发起⽅），因为开启了该功能，在调⽤ connect()函数时，内核会随机找⼀个 time_wait 状态超过 1 秒的连接给新的连接复⽤。**

net.ipv4.tcp_tw_reuse = 1


使⽤这个选项，还有⼀个前提，需要打开对 TCP 时间戳的⽀持，即

net.ipv4.tcp_timestamps=1（默认即为 1）


这个时间戳的字段是在 TCP 头部的「选项」⾥，⽤于记录 TCP 发送⽅的当前时间戳和从对端接收到的最新时间戳。

由于引⼊了时间戳，我们在前⾯提到的 2MSL 问题就不复存在了，因为重复的数据包会因为时间戳过期被⾃然丢弃。

- 方式二：net.ipv4.tcp_max_tw_buckets

这个值默认为 18000，当系统中处于 TIME_WAIT 的连接⼀旦超过这个值时，系统就会将后⾯的 TIME_WAIT 连接状态重置。

这个⽅法过于暴⼒，⽽且治标不治本，带来的问题远⽐解决的问题多，不推荐使⽤。



- 方式三：程序中使⽤ SO_LINGER ，应⽤强制使⽤ RST 关闭。

我们可以通过设置 socket 选项，来设置调⽤ close 关闭连接⾏为。

```cpp
struct linger so_linger;
so_linger.l_onoff = 1;
so_linger.l_linger = 0;
setsockopt(s, SOL_SOCKET, SO_LINGER, &so_linger,sizeof(so_linger));
```
如果 l_onoff 为⾮ 0， 且 l_linger 值为 0，那么调⽤ close 后，会⽴该发送⼀个 RST 标志给对端，该 TCP 连接将跳过四次挥⼿，也就跳过了 TIME_WAIT 状态，直接关闭。

但这为跨越 TIME_WAIT 状态提供了⼀个可能，不过是⼀个⾮常危险的⾏为，不值得提倡。



### 3.23 如果已经建⽴了连接，但是客户端突然出现故障了怎么办？

TCP 有⼀个机制是**保活机制**。这个机制的原理是这样的：

定义⼀个时间段，在这个时间段内，如果没有任何连接相关的活动，TCP 保活机制会开始作⽤，每隔⼀个时间间隔，发送⼀个探测报⽂，该探测报⽂包含的数据⾮常少，如果连续⼏个探测报⽂都没有得到响应，则认为当前的TCP 连接已经死亡，系统内核将错误信息通知给上层应⽤程序。

在 Linux 内核可以有对应的参数可以设置保活时间、保活探测的次数、保活探测的时间间隔，以下都为默认值：
```cpp
net.ipv4.tcp_keepalive_time=7200
net.ipv4.tcp_keepalive_intvl=75 
net.ipv4.tcp_keepalive_probes=9
```
- tcp_keepalive_time=7200：表示保活时间是 7200 秒（2⼩时），也就 2 ⼩时内如果没有任何连接相关的活动，则会启动保活机制.

- tcp_keepalive_intvl=75：表示每次检测间隔 75 秒；

- tcp_keepalive_probes=9：表示检测 9 次⽆响应，认为对⽅是不可达的，从⽽中断本次的连接。


也就是说在 Linux 系统中，最少需要经过 2 ⼩时 11 分 15 秒才可以发现⼀个「死亡」连接。

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211209152415.png)

这个时间是有点⻓的，我们也可以根据实际的需求，对以上的保活相关的参数进⾏设置。

如果开启了 TCP 保活，需要考虑以下⼏种情况：

第⼀种，对端程序是正常⼯作的。当 TCP 保活的探测报⽂发送给对端, 对端会正常响应，这样 TCP 保活时间会被重置，等待下⼀个 TCP 保活时间的到来。


第⼆种，对端程序崩溃并重启。当 TCP 保活的探测报⽂发送给对端后，对端是可以响应的，但由于没有该连接的有效信息，会产⽣⼀个 RST 报⽂，这样很快就会发现 TCP 连接已经被重置。

第三种，是对端程序崩溃，或对端由于其他原因导致报⽂不可达。当 TCP 保活的探测报⽂发送给对端后，⽯沉⼤海，没有响应，连续⼏次，达到保活探测次数后，TCP 会报告该 TCP 连接已经死亡。


### 3.24 针对TCP应该如何Socket编程？

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211209152928.png)


- 服务端和客户端初始化 socket ，得到⽂件描述符；

- 服务端调⽤ bind ，将绑定在 IP 地址和端⼝;

- 服务端调⽤ listen ，进⾏监听；

- 服务端调⽤ accept ，等待客户端连接；

- 客户端调⽤ connect ，向服务器端的地址和端⼝发起连接请求；

- 服务端 accept 返回⽤于传输的 socket 的⽂件描述符；

- 客户端调⽤ write 写⼊数据；服务端调⽤ read 读取数据；

- 客户端断开连接时，会调⽤ close ，那么服务端 read 读取数据的时候，就会读取到了 EOF ，待处理完
数据后，服务端调⽤ close ，表示连接关闭。


这⾥需要注意的是，服务端调⽤ accept 时，连接成功了会返回⼀个已完成连接的 socket，后续⽤来传输数据。

所以，监听的 socket 和真正⽤来传送数据的 socket，是「两个」 socket，⼀个叫作监听 socket，⼀个叫作已完成连接 socket。


成功连接建⽴之后，双⽅开始通过 read 和 write 函数来读写数据，就像往⼀个⽂件流⾥⾯写东⻄⼀样。

### 3.25 listen 时候参数 backlog 的意义？
Linux内核中会维护两个队列：

- 未完成连接队列（SYN 队列）：接收到⼀个 SYN 建⽴连接请求，处于 SYN_RCVD 状态；
- 已完成连接队列（Accpet 队列）：已完成 TCP 三次握⼿过程，处于 ESTABLISHED 状态；

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211209155408.png)


```cpp
int listen (int socketfd, int backlog)
```

- 参数⼀ socketfd 为 socketfd ⽂件描述符
- 参数⼆ backlog，这参数在历史版本有⼀定的变化


在早期 Linux 内核 backlog 是 SYN 队列⼤⼩，也就是未完成的队列⼤⼩。


在 Linux 内核 2.2 之后，backlog 变成 accept 队列，也就是已完成连接建⽴的队列⻓度，所以现在通常认为**backlog 是 accept 队列。**

**但是上限值是内核参数 somaxconn 的⼤⼩，也就说 accpet 队列⻓度 = min(backlog, somaxconn)。**


### 3.26 accept 发⽣在三次握⼿的哪⼀步？

我们先看看客户端连接服务端时，发送了什么？
![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211209160100.png)

- 客户端的协议栈向服务器端发送了 SYN 包，并告诉服务器端当前发送序列号 client_isn，客户端进⼊
SYN_SENT 状态；


- 服务器端的协议栈收到这个包之后，和客户端进⾏ ACK 应答，应答的值为 client_isn+1，表示对 SYN 包client_isn 的确认，同时服务器也发送⼀个 SYN 包，告诉客户端当前我的发送序列号为 server_isn，服务器端进⼊ SYN_RCVD 状态；


- 客户端协议栈收到 ACK 之后，使得应⽤程序从 connect 调⽤返回，表示客户端到服务器端的单向连接建⽴成功，客户端的状态为 ESTABLISHED，同时客户端协议栈也会对服务器端的 SYN 包进⾏应答，应答数据为
server_isn+1；


- 应答包到达服务器端后，服务器端协议栈使得 accept 阻塞调⽤返回，这个时候服务器端到客户端的单向连接也建⽴成功，服务器端也进⼊ ESTABLISHED 状态。


**从上⾯的描述过程，我们可以得知客户端 connect 成功返回是在第⼆次握⼿，服务端 accept 成功返回是在三次握⼿成功之后。**



### 3.27 客户端调⽤ close 了，连接是断开的流程是什么？
我们看看客户端主动调⽤了 close ，会发⽣什么？

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211209162019.png)


- 客户端调⽤ close ，表明客户端没有数据需要发送了，则此时会向服务端发送 FIN 报⽂，进⼊ FIN_WAIT_1状态；

- 服务端接收到了 FIN 报⽂，TCP 协议栈会为 FIN 包插⼊⼀个⽂件结束符 EOF 到接收缓冲区中，应⽤程序可以通过 read 调⽤来感知这个 FIN 包。这个 EOF 会被放在已排队等候的其他已接收的数据之后，这就
意味着服务端需要处理这种异常情况，因为 EOF 表示在该连接上再⽆额外数据到达。此时，服务端进⼊
CLOSE_WAIT 状态；

- 接着，当处理完数据后，⾃然就会读到 EOF ，于是也调⽤ close 关闭它的套接字，这会使得客户端会发
出⼀个 FIN 包，之后处于 LAST_ACK 状态；

- 客户端接收到服务端的 FIN 包，并发送 ACK 确认包给服务端，此时客户端将进⼊ TIME_WAIT 状态；

- 服务端收到 ACK 确认包后，就进⼊了最后的 CLOSE 状态；

- 客户端经过 2MSL 时间之后，也进⼊ CLOSE 状态；

### 3.28 为了⽅便调试服务器程序，⼀般会在服务端设置 SO_REUSEADDR 选项，这样服务器程序在启后，可以⽴刻使⽤。这⾥设置SO_REUSEADDR 是不是就等价于对这个 socket 设置了内核中的net.ipv4.tcp_tw_reuse=1 这个选项？”

这两个东⻄没有关系的哦。

- 1. tcp_tw_reuse 是内核选项，主要⽤在连接的发起⽅（客户端）。TIME_WAIT 状态的连接创建时间超过 1 秒后，新的连接才可以被复⽤，注意，这⾥是「连接的发起⽅」；


- 2. SO_REUSEADDR 是⽤户态的选项，⽤于「连接的服务⽅」，⽤来告诉操作系统内核，如果端⼝已被占⽤，但是 TCP 连接状态位于 TIME_WAIT ，可以重⽤端⼝。如果端⼝忙，⽽ TCP 处于其他状态， 重⽤会有
“Address already in use” 的错误信息。


tcp_tw_reuse 是为了缩短 time_wait 的时间，避免出现⼤量的 time_wait 连接⽽占⽤系统资源，解决的是 accept后的问题。


SO_REUSEADDR 是为了解决 time_wait 状态带来的端⼝占⽤问题，以及⽀持同⼀个 port 对应多个 ip，解决的是bind 时的问题。


### 3.29 如果客户端第四次挥⼿ack丢失，服务端超时重发的fin报⽂也丢失，客户端timewait时间超过了2msl，这个时候会发⽣什么？认为连接已经关闭吗？”

当客户端 timewait 时间超过了 2MSL，则客户端就直接进⼊关闭状态。

服务端超时重发 fin 报⽂的次数如果超过 tcp_orphan_retries ⼤⼩后，服务端也会关闭 TCP 连接。


### 3.30  ⽂章在解释IP分⽚和TCP MSS分⽚时说，如果⽤IP分⽚会有两个问题：（1）IP按MTU分⽚，如果某⼀⽚丢失则需要所有分⽚都重传；（2）IP没有重传机制，所以需要等TCP发送⽅超时才能重传；问题⼀：MSS跟IP的MTU分⽚相⽐，只是多了⼀步协商MSS值的过程，⽽IP的MTU可以看作是默认协商好就是1500字节，所以为什么协商后的MSS可以做到丢失后只发丢失的这⼀⽚来提⾼效率，⽽默认协商好1500字节的IP分⽚就需要所有⽚都重传呢？问题⼆：TCP MSS分⽚如果丢失了⼀⽚，是不是也需要发送⽅等待超时再重传？如果不是，MSS的协商如何能在超时前就直到丢了分⽚从⽽提⾼效率的呢？

问题⼀：

如果⼀个⼤的 TCP 报⽂是被 MTU 分⽚，那么只有「第⼀个分⽚」才具有 TCP 头部，后⾯的分⽚则没有
TCP 头部，接收⽅ IP 层只有重组了这些分⽚，才会认为是⼀个 TCP 报⽂，那么丢失了其中⼀个分⽚，接收
⽅ IP 层就不会把 TCP 报⽂丢给 TCP 层，那么就会等待对⽅超时重传这⼀整个 TCP 报⽂。

如果⼀个⼤的 TCP 报⽂被 MSS 分⽚，那么所有「分⽚都具有 TCP 头部」，因为每个 MSS 分⽚的是具有
TCP 头部的TCP报⽂，那么其中⼀个 MSS 分⽚丢失，就只需要重传这⼀个分⽚就可以。

问题⼆：

TCP MSS分⽚如果丢失了⼀⽚，发送⽅没收到对⽅ACK应答，也是会触发超时重传的，因为TCP层是会保证
数据的可靠交付。




### 3.31 如果是服务提供⽅发起的 close ，然后引起过多的 time_wait 状态的 tcp 链接，time_wait 会影响服务端的端⼝吗？

不会。如果发起连接⼀⽅（客户端）的 TIME_WAIT 状态过多，占满了所有端⼝资源，则会导致⽆法创建新连接

客户端受端⼝资源限制：

- 客户端TIME_WAIT过多，就会导致端⼝资源被占⽤，因为端⼝就65536个，被占满就会导致⽆法创建新连
接。

服务端受系统资源限制：

- 由于⼀个 TCP 四元组表示 TCP 连接，理论上服务端可以建⽴很多连接，服务端只监听⼀个端⼝，但是会把连接扔给处理线程，所以理论上监听的端⼝可以继续监听。但是线程池处理不了那么多⼀直不断的连接了。所以当服务端出现⼤量 TIMEWAIT 时，系统资源容易被耗尽。




![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211209182013.png)


### 3.32 重传机制

TCP 实现可靠传输的⽅式之⼀，是通过序列号与确认应答。


在 TCP 中，当发送端的数据到达接收主机时，接收端主机会返回⼀个**确认应答消息，表示已收到消息。**

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211209204507.png)

但在错综复杂的⽹络，并不⼀定能如上图那么顺利能正常的数据传输，万⼀数据在传输过程中丢失了呢？

所以 TCP 针对数据包丢失的情况，会⽤**重传机制**解决。

接下来说说常⻅的重传机制：

- 超时重传
- 快速重传
- SACK
- D-SACK

#### 超时重传

重传机制的其中⼀个⽅式，就是在发送数据时，设定⼀个定时器，当超过指定的时间后，没有收到对⽅的 ACK确认应答报⽂，就会重发该数据，也就是我们常说的超时重传。

TCP 会在以下两种情况发⽣超时重传：

- 数据包丢失
- 确认应答丢失

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211209212628.png)

>超时时间应该设置为多少呢？</font>。

我们先来了解⼀下什么是 RTT （Round-Trip Time 往返时延），从下图我们就可以知道：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211209213710.png)

RTT 就是**数据从⽹络⼀端传送到另⼀端所需的时间，也就是包的往返时间。**

超时重传时间是以 RTO （Retransmission Timeout 超时重传时间）表示。

假设在重传的情况下，超时时间 RTO 「较⻓或较短」时，会发⽣什么事情呢？

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211209214106.png)

上图中有两种超时时间不同的情况：

- 当超时时间 RTO 较⼤时，重发就慢，丢了⽼半天才重发，没有效率，性能差；
- 当超时时间 RTO 较⼩时，会导致可能并没有丢就重发，于是重发的就快，会增加⽹络拥塞，导致更多的超
时，更多的超时导致更多的重发。


精确的测量超时时间 RTO 的值是⾮常重要的，这可让我们的重传机制更⾼效。
根据上述的两种情况，我们可以得知，超时重传时间 RTO 的值应该略⼤于报⽂往返 RTT 的值。


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211209214832.png)

⾄此，可能⼤家觉得超时重传时间 RTO 的值计算，也不是很复杂嘛。


好像就是在发送端发包时记下 t0 ，然后接收端再把这个 ack 回来时再记⼀个 t1 ，于是 RTT = t1 – t0 。没那么简单，这只是⼀个采样，不能代表普遍情况。

实际上「报⽂往返 RTT 的值」是经常变化的，因为我们的⽹络也是时常变化的。也就因为「报⽂往返 RTT 的值」是经常波动变化的，所以「超时重传时间 RTO 的值」应该是⼀个动态变化的值。

我们来看看 Linux 是如何计算 RTO 的呢？

估计往返时间，通常需要采样以下两个：

- 需要 TCP 通过采样 RTT 的时间，然后进⾏加权平均，算出⼀个平滑 RTT 的值，⽽且这个值还是要不断变化的，因为⽹络状况不断地变化。

- 除了采样 RTT，还要采样 RTT 的波动范围，这样就避免如果 RTT 有⼀个⼤的波动的话，很难被发现的情况。


RFC6289 建议使⽤以下的公式计算 RTO：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211209220900.png)


其中 SRTT 是计算平滑的RTT ， DevRTR 是计算平滑的RTT 与 最新 RTT 的差距。

在 Linux 下，α = 0.125，β = 0.25， μ = 1，∂ = 4。别问怎么来的，问就是⼤量实验中调出来的。

如果超时重发的数据，再次超时的时候，⼜需要重传的时候，TCP 的策略是超时间隔加倍。

也就是每当遇到⼀次超时重传的时候，都会将下⼀次超时时间间隔设为先前值的两倍。两次超时，就说明⽹络环境差，不宜频繁反复发送。

超时触发重传存在的问题是，超时周期可能相对较⻓。那是不是可以有更快的⽅式呢？

于是就可以⽤「快速重传」机制来解决超时重发的时间等待。



#### 快速重传
TCP 还有另外⼀种快速重传（Fast Retransmit）机制，它不以时间为驱动，⽽是以数据驱动重传。

快速重传机制，是如何⼯作的呢？其实很简单，⼀图胜千⾔。

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211209222157.png)


在上图，发送⽅发出了 1，2，3，4，5 份数据：


- 第⼀份 Seq1 先送到了，于是就 Ack 回 2；

- 结果 Seq2 因为某些原因没收到，Seq3 到达了，于是还是 Ack 回 2；

- 后⾯的 Seq4 和 Seq5 都到了，但还是 Ack 回 2，因为 Seq2 还是没有收到；

- 发送端收到了三个 Ack = 2 的确认，知道了 Seq2 还没有收到，就会在定时器过期之前，重传丢失的 Seq2。

- 最后，收到了 Seq2，此时因为 Seq3，Seq4，Seq5 都收到了，于是 Ack 回 6 。


所以，快速重传的⼯作⽅式是当收到三个相同的 ACK 报⽂时，会在定时器过期之前，重传丢失的报⽂段。

快速重传机制只解决了⼀个问题，就是超时时间的问题，但是它依然⾯临着另外⼀个问题。就是重传的时候，是重传之前的⼀个，还是重传所有的问题。


⽐如对于上⾯的例⼦，是重传 Seq2 呢？还是重传 Seq2、Seq3、Seq4、Seq5 呢？因为发送端并不清楚这连续的三个 Ack 2 是谁传回来的。


根据 TCP 不同的实现，以上两种情况都是有可能的。可⻅，这是⼀把双刃剑。


为了解决不知道该重传哪些 TCP 报⽂，于是就有 SACK ⽅法。



#### SACK ⽅法

还有⼀种实现重传机制的⽅式叫： SACK （ Selective Acknowledgment 选择性确认）。


这种⽅式需要在 TCP 头部「选项」字段⾥加⼀个 **SACK** 的东⻄，它可以**将缓存的地图**发送给发送⽅，这样发送⽅就可以知道哪些数据收到了，哪些数据没收到，知道了这些信息，就可以只重传丢失的数据。

如下图，发送⽅收到了三次同样的 ACK 确认报⽂，于是就会触发快速重发机制，通过 SACK 信息发现只有
200~299 这段数据丢失，则重发时，就只选择了这个 TCP 段进⾏重发。





![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211209223723.png)


如果要⽀持 SACK ，必须双⽅都要⽀持。在 Linux 下，可以通过 net.ipv4.tcp_sack 参数打开这个功能（Linux2.4 后默认打开）。

#### Duplicate SACK
Duplicate SACK ⼜称 D-SACK ，其主要使⽤了 SACK 来告诉「发送⽅」有哪些数据被重复接收了。

下⾯举例两个栗⼦，来说明 D-SACK 的作⽤。
![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211209224547.png)


- 「接收⽅」发给「发送⽅」的两个 ACK 确认应答都丢失了，所以发送⽅超时后，重传第⼀个数据包（3000 ~3499）

- 于是「接收⽅」发现数据是重复收到的，于是回了⼀个 SACK = 3000~3500，告诉「发送⽅」 3000~3500
的数据早已被接收了，因为 ACK 都到了 4000 了，已经意味着 4000 之前的所有数据都已收到，所以这个
SACK 就代表着 D-SACK 。


- 这样「发送⽅」就知道了，数据没有丢，是「接收⽅」的 ACK 确认报⽂丢了。

栗⼦⼆号：⽹络延时

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211209224744.png)


- 数据包（1000~1499） 被⽹络延迟了，导致「发送⽅」没有收到 Ack 1500 的确认报⽂。


- ⽽后⾯报⽂到达的三个相同的 ACK 确认报⽂，就触发了快速 传机制，但是在 传后，被延迟的数据包
（1000~1499）⼜到了「接收⽅」；


- 所以「接收⽅」回了⼀个 SACK=1000~1500，因为 ACK 已经到了 3000，所以这个 SACK 是 D-SACK，表
示收到了重复的包。


- 这样发送⽅就知道快速重传触发的原因不是发出去的包丢了，也不是因为回应的 ACK 包丢了，⽽是因为⽹络延迟了。


可⻅， D-SACK 有这么⼏个好处：

1. 可以让「发送⽅」知道，是发出去的包丢了，还是接收⽅回应的 ACK 包丢了;
2. 可以知道是不是「发送⽅」的数据包被⽹络延迟了;
3. 可以知道⽹络中是不是把「发送⽅」的数据包给复制了;

在 Linux 下可以通过 net.ipv4.tcp_dsack 参数开启/关闭这个功能（Linux 2.4 后默认打开）。


### 3.33 滑动窗⼝

我们都知道 TCP 是每发送⼀个数据，都要进⾏⼀次确认应答。当上⼀个数据包收到了应答了， 再发送下⼀个。但这种⽅式的缺点是效率⽐较低。


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211210172757.png)

所以，这样的传输⽅式有⼀个缺点：**数据包的往返时间越⻓，通信的效率就越低。**

为解决这个问题，TCP 引⼊了**窗⼝**这个概念。即使在往返时间较⻓的情况下，它也不会降低⽹络通信的效率。


那么有了窗⼝，就可以指定窗⼝⼤⼩，窗⼝⼤⼩就是指 **⽆需等待确认应答，⽽可以继续发送数据的最⼤值**

窗⼝的实现实际上是操作系统开辟的⼀个缓存空间，发送⽅主机在等待确认应答返回之前，必须在缓冲区中保留已发送的数据。如果按期收到确认应答，此时数据就可以从缓存区清除。

假设窗⼝⼤⼩为 3 个 TCP 段，那么发送⽅就可以「连续发送」 3 个 TCP 段，并且中途若有 ACK 丢失，可以通过「下⼀个确认应答进⾏确认」。如下图：



![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211210174319.png)


图中的 ACK 600 确认应答报⽂丢失，也没关系，因为可以通过下⼀个确认应答进⾏确认，只要发送⽅收到了 ACK 700 确认应答，就意味着 700 之前的所有数据「接收⽅」都收到了。这个模式就叫**累计确认**或者**累计应答**。

### 3.34 窗⼝⼤⼩由哪⼀⽅决定？
TCP 头⾥有⼀个字段叫 Window ，也就是窗⼝⼤⼩。

这个字段是接收端告诉发送端⾃⼰还有多少缓冲区可以接收数据。于是发送端就可以根据这个接收端的处理能⼒来发送数据，⽽不会导致接收端处理不过来。

所以，通常窗⼝的⼤⼩是由**接收⽅的窗⼝⼤⼩**来决定的。

发送⽅发送的数据⼤⼩不能超过接收⽅的窗⼝⼤⼩，否则接收⽅就⽆法正常接收到数据。

### 3.35 发送⽅的滑动窗⼝

我们先来看看发送⽅的窗⼝，下图就是发送⽅缓存的数据，根据处理的情况分成四个部分，其中深蓝⾊⽅框是发送窗⼝，紫⾊⽅框是可⽤窗⼝：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211210180650.png)

- #1 是已发送并收到 ACK确认的数据：1~31 字节

- #2 是已发送但未收到 ACK确认的数据：32~45 字节

- #3 是未发送但总⼤⼩在接收⽅处理范围内（接收⽅还有空间）：46~51字节

- #4 是未发送但总⼤⼩超过接收⽅处理范围（接收⽅没有空间）：52字节以后


在下图，当发送⽅把数据「全部」都⼀下发送出去后，可⽤窗⼝的⼤⼩就为 0 了，表明可⽤窗⼝耗尽，在没收到ACK 确认之前是⽆法继续发送数据了。

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211210182122.png)


在下图，当收到之前发送的数据 32~36 字节的 ACK 确认应答后，如果发送窗⼝的⼤⼩没有变化，则**滑动窗⼝往右边移动 5 个字节，因为有 5 个字节的数据被应答确认**，接下来 52~56 字节⼜变成了可⽤窗⼝，那么后续也就可以发送 52~56 这 5 个字节的数据了。

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211210182949.png)


### 3.36 程序是如何表示发送⽅的四个部分的呢？

TCP 滑动窗⼝⽅案使⽤三个指针来跟踪在四个传输类别中的每⼀个类别中的字节。其中两个指针是绝对指针（指特定的序列号），⼀个是相对指针（需要做偏移）。

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211210183536.png)


- SND.WND ：表示发送窗⼝的⼤⼩（⼤⼩是由接收⽅指定的）；

- SND.UNA ：是⼀个绝对指针，它指向的是已发送但未收到确认的第⼀个字节的序列号，也就是 #2 的第⼀
个字节。


- SND.NXT ：也是⼀个绝对指针，它指向未发送但可发送范围的第⼀个字节的序列号，也就是 #3 的第⼀个
字节。


- 指向 #4 的第⼀个字节是个相对指针，它需要 SND.UNA 指针加上 SND.WND ⼤⼩的偏移 ，就可以指向
#4 的第⼀个字节了。


那么可⽤窗⼝⼤⼩的计算就可以是：


可⽤窗⼝⼤ = SND.WND -（SND.NXT - SND.UNA）

### 3.37 接收⽅的滑动窗⼝

接下来我们看看接收⽅的窗⼝，接收窗⼝相对简单⼀些，根据处理的情况划分成三个部分：

- #1 + #2 是已成功接收并确认的数据（等待应⽤进程读取）；

- #3 是未收到数据但可以接收的数据；

- #4 未收到数据并不可以接收的数据；

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211210183536.png)


其中三个接收部分，使⽤两个指针进⾏划分:
- RCV.WND ：表示接收窗⼝的⼤⼩，它会通告给发送⽅。
- RCV.NXT ：是⼀个指针，它指向期望从发送⽅发送来的下⼀个数据字节的序列号，也就是 #3 的第⼀个字
节。

- 指向 #4 的第⼀个字节是个相对指针，它需要 RCV.NXT 指针加上 RCV.WND ⼤⼩的偏移 ，就可以指向
#4 的第⼀个字节了。


### 3.38 接收窗⼝和发送窗⼝的⼤⼩是相等的吗？
并不是完全相等，接收窗⼝的⼤⼩是**约等于**发送窗⼝的⼤⼩的。

因为滑动窗⼝并不是⼀成不变的。⽐如，当接收⽅的应⽤进程读取数据的速度⾮常快的话，这样的话接收窗⼝可以、很快的就空缺出来。那么新的接收窗⼝⼤⼩，是通过 TCP 报⽂中的 Windows 字段来告诉发送⽅。那么这个传输过程是存在时延的，所以接收窗⼝和发送窗⼝是约等于的关系。


### 3.39 流量控制
发送⽅不能⽆脑的发数据给接收⽅，要考虑接收⽅处理能⼒。

如果⼀直⽆脑的发数据给对⽅，但对⽅处理不过来，那么就会导致触发重发机制，从⽽导致⽹络流量的⽆端的浪费。


为了解决这种现象发⽣，**TCP 提供⼀种机制可以让「发送⽅」根据「接收⽅」的实际接收能⼒控制发送的数据量，这就是所谓的流量控制。**

下⾯举个栗⼦，为了简单起⻅，假设以下场景：

- 客户端是接收⽅，服务端是发送⽅

- 假设接收窗⼝和发送窗⼝相同，都为 200

- 假设两个设备在整个传输过程中都保持相同的窗⼝⼤⼩，不受外界影响


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211210191027.png)



![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211210191143.png)


- 1、客户端向服务端发送请求数据报⽂。这⾥要说明下，本次例⼦是把服务端作为发送⽅，所以没有画出服务端的接收窗⼝。

- 2、服务端收到请求报⽂后，发送确认报⽂和 80 字节的数据，于是可⽤窗⼝ Usable 减少为 120 字节，同时SND.NXT 指针也向右偏移 80 字节后，指向 321，这意味着下次发送数据的时候，序列号是 321。

- 3、客户端收到 80 字节数据后，于是接收窗⼝往右移动 80 字节， RCV.NXT 也就指向 321，这意味着客户端期望的下⼀个报⽂的序列号是 321，接着发送确认报⽂给服务端。

- 4、服务端再次发送了 120 字节数据，于是可⽤窗⼝耗尽为 0，服务端⽆法再继续发送数据。

- 5、客户端收到 120 字节的数据后，于是接收窗⼝往右移动 120 字节， RCV.NXT 也就指向 441，接着发送确认报⽂给服务端。


- 6、服务端收到对 80 字节数据的确认报⽂后， SND.UNA 指针往右偏移后指向 321，于是可⽤窗⼝ Usable增⼤到 80。


- 7 、服务端收到对 120 字节数据的确认报⽂后， SND.UNA 指针往右偏移后指向 441，于是可⽤窗⼝ Usable增⼤到 200。

- 8、 服务端可以继续发送了，于是发送了 160 字节的数据后， SND.NXT 指向 601，于是可⽤窗⼝
Usable 减少到 40。

- 9、客户端收到 160 字节后，接收窗⼝往右移动了 160 字节， RCV.NXT 也就是指向了 601，接着发送确认报⽂给服务端。


- 10、服务端收到对 160 字节数据的确认报⽂后，发送窗⼝往右移动了 160 字节，于是 SND.UNA 指针偏移了160 后指向 601，可⽤窗⼝ Usable 也就增⼤⾄了 200。



#### 操作系统缓冲区与滑动窗⼝的关系
前⾯的流量控制例⼦，我们假定了发送窗⼝和接收窗⼝是不变的，但是实际上，发送窗⼝和接收窗⼝中所存放的字节数，都是放在操作系统内存缓冲区中的，⽽操作系统的缓冲区，会被操作系统调整。

当应⽤进程没办法及时读取缓冲区的内容时，也会对我们的缓冲区造成影响。

那操作系统的缓冲区，是如何影响发送窗⼝和接收窗⼝的呢？

我们先来看看第⼀个例⼦。

当应⽤程序没有及时读取缓存时，发送窗⼝和接收窗⼝的变化。

考虑以下场景：

- 客户端作为发送⽅，服务端作为接收⽅，发送窗⼝和接收窗⼝初始⼤⼩为 360 ；

- 服务端⾮常的繁忙，当收到客户端的数据时，应⽤层不能及时读取数据。



![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211210192922.png)


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211210192925.png)


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211210192928.png)



根据上图的流量控制，说明下每个过程：

1. 客户端发送 140 字节数据后，可⽤窗⼝变为 220 （360 - 140）。

2. 服务端收到 140 字节数据，但是服务端⾮常繁忙，应⽤进程只读取了 40 个字节，还有 100 字节占⽤着缓冲区，于是接收窗⼝收缩到了 260 （360 - 100），最后发送确认信息时，将窗⼝⼤⼩通告给客户端。

3. 客户端收到确认和窗⼝通告报⽂后，发送窗⼝减少为 260。

4. 客户端发送 180 字节数据，此时可⽤窗⼝减少到 80。

5. 服务端收到 180 字节数据，但是应⽤程序没有读取任何数据，这 180 字节直接就留在了缓冲区，于是接收窗⼝收缩到了 80 （260 - 180），并在发送确认信息时，通过窗⼝⼤⼩给客户端。

6. 客户端收到确认和窗⼝通告报⽂后，发送窗⼝减少为 80。

7. 客户端发送 80 字节数据后，可⽤窗⼝耗尽。

8. 服务端收到 80 字节数据，但是应⽤程序依然没有读取任何数据，这 80 字节留在了缓冲区，于是接收窗⼝收缩到了 0，并在发送确认信息时，通过窗⼝⼤⼩给客户端。

9. 客户端收到确认和窗⼝通告报⽂后，发送窗⼝减少为 0。


可⻅最后窗⼝都收缩为 0 了，也就是发⽣了窗⼝关闭。当发送⽅可⽤窗⼝变为 0 时，发送⽅实际上会定时发送窗⼝探测报⽂，以便知道接收⽅的窗⼝是否发⽣了改变，这个内容后⾯会说，这⾥先简单提⼀下。


**我们先来看看第⼆个例⼦。**

当服务端系统资源⾮常紧张的时候，操作系统可能会直接减少接收缓冲区⼤⼩，这时应⽤程序⼜⽆法及时读取缓存数据，那么这时候就有严重的事情发⽣了，会出现数据包丢失的现象。

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211210194530.png)


说明下每个过程：

1. 客户端发送 140 字节的数据，于是可⽤窗⼝减少到了 220。

2. 服务端因为现在⾮常的繁忙，操作系统于是就把接收缓存减少了 120 字节，当收到 140 字节数据后，⼜因为应⽤程序没有读取任何数据，所以 140 字节留在了缓冲区中，于是接收窗⼝⼤⼩从 360 收缩成了 100，最后发送确认信息时，通告窗⼝⼤⼩给对⽅。

3. 此时客户端因为还没有收到服务端的通告窗⼝报⽂，所以不知道此时接收窗⼝收缩成了 100，客户端只会看⾃⼰的可⽤窗⼝还有 220，所以客户端就发送了 180 字节数据，于是可⽤窗⼝减少到 40。

4. 服务端收到了 180 字节数据时，发现数据⼤⼩超过了接收窗⼝的⼤⼩，于是就把数据包丢失了。

5. 客户端收到第 2 步时，服务端发送的确认报⽂和通告窗⼝报⽂，尝试减少发送窗⼝到 100，把窗⼝的右端向左收缩了 80，此时可⽤窗⼝的⼤⼩就会出现诡异的负值。

所以，如果发⽣了先减少缓存，再收缩窗⼝，就会出现丢包的现象。

**为了防⽌这种情况发⽣，TCP 规定是不允许同时减少缓存⼜收缩窗⼝的，⽽是采⽤先收缩窗⼝，过段时间再减少缓存，这样就可以避免了丢包情况。**



#### 窗⼝关闭

在前⾯我们都看到了，TCP 通过让接收⽅指明希望从发送⽅接收的数据⼤⼩（窗⼝⼤⼩）来进⾏流量控制。

如果窗⼝⼤⼩为 0 时，就会阻⽌发送⽅给接收⽅传递数据，直到窗⼝变为d⾮ 0 为⽌，这就是窗⼝关闭。


### 3.40 窗⼝关闭潜在的危险

接收⽅向发送⽅通告窗⼝⼤⼩时，是通过 ACK 报⽂来通告的。

那么，当发⽣窗⼝关闭时，接收⽅处理完数据后，会向发送⽅通告⼀个窗⼝⾮ 0 的 ACK 报⽂，如果这个通告窗⼝的 ACK 报⽂在⽹络中丢失了，那麻烦就⼤了。

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211210201109.png)


这会导致发送⽅⼀直等待接收⽅的⾮ 0 窗⼝通知，接收⽅也⼀直等待发送⽅的数据，如不采取措施，这种相互等待的过程，会造成了死锁的现象。


### 3.41 TCP 是如何解决窗⼝关闭时，潜在的死锁现象呢？

为了解决这个问题，TCP 为每个连接设有⼀个持续定时器，只要 TCP 连接⼀⽅收到对⽅的零窗⼝通知，就启动持续计时器。

如果持续计时器超时，就会发送窗⼝探测 ( Windowprobe ) 报⽂，⽽对⽅在确认这个探测报⽂时，给出⾃⼰现在的接收窗⼝⼤⼩。


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211210202534.png)


- 如果接收窗⼝仍然为 0，那么收到这个报⽂的⼀⽅就会重新启动持续计时器；

- 如果接收窗⼝不是 0，那么死锁的局⾯就可以被打破了。


窗⼝探测的次数⼀般为 3 次，每次⼤约 30-60 秒（不同的实现可能会不⼀样）。如果 3 次过后接收窗⼝还是 0 的话，有的 TCP 实现就会发 RST 报⽂来中断连接。


#### 糊涂窗⼝综合症

如果接收⽅太忙了，来不及取⾛接收窗⼝⾥的数据，那么就会导致发送⽅的发送窗⼝越来越⼩。

到最后，如果接收⽅腾出⼏个字节并告诉发送⽅现在有⼏个字节的窗⼝，⽽发送⽅会义⽆反顾地发送这⼏个字节，这就是糊涂窗⼝综合症。


要知道，我们的 TCP + IP 头有 40 个字节，为了传输那⼏个字节的数据，要搭上这么⼤的开销，这太不经济了。

就好像⼀个可以承载 50 ⼈的⼤巴⻋，每次来了⼀两个⼈，就直接发⻋。除⾮家⾥有矿的⼤巴司机，才敢这样玩，不然迟早破产。要解决这个问题也不难，⼤巴司机等乘客数 超过了 25 个，才认定可以发⻋。

现举个糊涂窗⼝综合症的栗⼦，考虑以下场景：
接收⽅的窗⼝⼤⼩是 360 字节，但接收⽅由于某些原因陷⼊困境，假设接收⽅的应⽤层读取的能⼒如下：

- 接收⽅每接收 3 个字节，应⽤程序就只能从缓冲区中读取 1 个字节的数据；

- 在下⼀个发送⽅的 TCP 段到达之前，应⽤程序还从缓冲区中读取了 40 个额外的字节；


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211210204725.png)


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211210204802.png)

每个过程的窗⼝⼤⼩的变化，在图中都描述的很清楚了，可以发现窗⼝不断减少了，并且发送的数据都是⽐较⼩的了。

所以，糊涂窗⼝综合症的现象是可以发⽣在发送⽅和接收⽅：

- 接收⽅可以通告⼀个⼩的窗⼝
- ⽽发送⽅可以发送⼩数据

于是，要解决糊涂窗⼝综合症，就解决上⾯两个问题就可以了

- 接收⽅不通告⼩窗⼝给发送⽅
- 让发送⽅避免发送⼩数据




### 3.42 怎么让接收⽅不通告⼩窗⼝呢？

接收⽅通常的策略如下:


当「窗⼝⼤⼩」⼩于 min( MSS，缓存空间/2 ) ，也就是⼩于 MSS 与 1/2 缓存⼤⼩中的最⼩值时，就会向发送⽅通告窗⼝为 0 ，也就阻⽌了发送⽅再发数据过来。

等到接收⽅处理了⼀些数据后，窗⼝⼤⼩ >= MSS，或者接收⽅缓存空间有⼀半可以使⽤，就可以把窗⼝打开让发送⽅发送数据过来。


### 3.43 怎么让发送⽅避免发送⼩数据呢？
发送⽅通常的策略:
使⽤ Nagle 算法，该算法的思路是延时处理，它满⾜以下两个条件中的⼀条才可以发送数据：

- 要等到窗⼝⼤⼩ >= MSS 或是 数据⼤⼩ >= MSS
- 收到之前发送数据的 ack 回包


只要没满⾜上⾯条件中的⼀条，发送⽅⼀直在囤积数据，直到满⾜上⾯的发送条件。



另外，Nagle 算法默认是打开的，如果对于⼀些需要⼩数据包交互的场景的程序，⽐如，telnet 或 ssh 这样的交互性⽐较强的程序，则需要关闭 Nagle 算法。


可以在 Socket 设置 TCP_NODELAY 选项来关闭这个算法（关闭 Nagle 算法没有全局参数，需要根据每个应⽤⾃⼰的特点来关闭）


```cpp
setsockopt(sock_fd, IPPROTO_TCP, TCP_NODELAY, (char *)&value, sizeof(int));
```


### 3.44 为什么要有拥塞控制呀，不是有流量控制了吗？

流量控制是避免「发送⽅」的数据填满「接收⽅」的缓存，但是并不知道⽹络的中发⽣了什么。


⼀般来说，计算机⽹络都处在⼀个共享的环境。因此也有可能会因为其他主机之间的通信使得⽹络拥堵。


在⽹络出现拥堵时，如果继续发送⼤量数据包，可能会导致数据包时延、丢失等，这时 TCP 就会重传数据，但是⼀重传就会导致⽹络的负担更重，于是会导致更⼤的延迟以及更多的丢包，这个情况就会进⼊恶性循环被不断地放⼤....


所以，TCP 不能忽略⽹络上发⽣的事，它被设计成⼀个⽆私的协议，当⽹络发送拥塞时，TCP 会⾃我牺牲，降低发送的数据量。


于是，就有了**拥塞控制**，控制的⽬的就是**避免「发送⽅」的数据填满整个⽹络。**

为了在「发送⽅」调节所要发送数据的量，定义了⼀个叫做「拥塞窗⼝」的概念。



### 3.45 什么是拥塞窗⼝？和发送窗⼝有什么关系呢？
**拥塞窗⼝ cwnd**是发送⽅维护的⼀个的状态变量，它会**根据⽹络的拥塞程度动态变化**的。

我们在前⾯提到过发送窗⼝ swnd 和接收窗⼝ rwnd 是约等于的关系，那么由于加⼊了拥塞窗⼝的概念后，此时**发送窗⼝的值是swnd = min(cwnd, rwnd)，也就是拥塞窗⼝和接收窗⼝中的最⼩值。**


拥塞窗⼝ cwnd 变化的规则：

- 只要⽹络中没有出现拥塞， cwnd 就会增⼤；

- 但⽹络中出现了拥塞， cwnd 就减少；


### 3.46 那么怎么知道当前⽹络是否出现了拥塞呢？

其实只要「发送⽅」没有在规定时间内接收到 ACK 应答报⽂，也就是发⽣了超时重传，就会认为⽹络出现了拥塞。



### 3.47 拥塞控制有哪些控制算法？

- 慢启动
- 拥塞避免
- 拥塞发⽣
- 快速恢复


#### 慢启动
TCP 在刚建⽴连接完成后，⾸先是有个慢启动的过程，这个慢启动的意思就是⼀点⼀点的提⾼发送数据包的数量，如果⼀上来就发⼤量的数据，这不是给⽹络添堵吗？


慢启动的算法记住⼀个规则就⾏：**当发送⽅每收到⼀个 ACK，拥塞窗⼝ cwnd 的⼤⼩就会加 1。**


这⾥假定拥塞窗⼝ cwnd 和发送窗⼝ swnd 相等，下⾯举个栗⼦：

- 连接建⽴完成后，⼀开始初始化 cwnd = 1 ，表示可以传⼀个 MSS ⼤⼩的数据。


- 当收到⼀个 ACK 确认应答后，cwnd 增加 1，于是⼀次能够发送 2 个


- 当收到 2 个的 ACK 确认应答后， cwnd 增加 2，于是就可以⽐之前多发2 个，所以这⼀次能够发送 4 个

- 当这 4 个的 ACK 确认到来的时候，每个确认 cwnd 增加 1， 4 个确认 cwnd 增加 4，于是就可以⽐之前多发4 个，所以这⼀次能够发送 8 个。



![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211105107.png)

可以看出慢启动算法，发包的个数是指数性的增⻓。

那慢启动涨到什么时候是个头呢？
有⼀个叫慢启动⻔限ssthresh （slow start threshold）状态变量

- 当 cwnd < ssthresh 时，使⽤慢启动算法。
- 当 cwnd >= ssthresh 时，就会使⽤「拥塞避免算法」。



#### 拥塞避免算法

前⾯说道，当拥塞窗⼝ cwnd 「超过」慢启动⻔限 ssthresh 就会进⼊拥塞避免算法。

一般来说 ssthresh 的⼤⼩是 65535 字节。

那么进⼊拥塞避免算法后，它的规则是：每当收到⼀个 ACK 时，cwnd 增加 1/cwnd。


接上前⾯的慢启动的栗⼦，现假定 ssthresh 为 8 ：


当 8 个 ACK 应答确认到来时，每个确认增加 1/8，8 个 ACK 确认 cwnd ⼀共增加 1，于是这⼀次能够发送 9个 MSS ⼤⼩的数据，变成了线性增⻓。


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211110414.png)


所以，我们可以发现，拥塞避免算法就是将原本慢启动算法的指数增⻓变成了线性增⻓，还是增⻓阶段，但是增⻓速度缓慢了⼀些。


就这么⼀直增⻓着后，⽹络就会慢慢进⼊了拥塞的状况了，于是就会出现丢包现象，这时就需要对丢失的数据包进⾏重传。


当触发了重传机制，也就进⼊了「拥塞发⽣算法」。


#### 拥塞发⽣
当⽹络出现拥塞，也就是会发⽣数据包重传，重传机制主要有两种：

- 超时重传
- 快速重传

这两种使⽤的拥塞发生算法是不同的，接下来分别来说说。


##### 发⽣超时重传的拥塞发⽣算法
这个时候，ssthresh 和 cwnd 的值会发⽣变化：

- ssthresh 设为 cwnd/2 ，
- cwnd 重置为 1


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211112212.png)


接着，就重新开始慢启动，慢启动是会突然减少数据流的。这真是⼀旦「超时重传」，⻢上回到解放前。但是这种⽅式太激进了，反应也很强烈，会造成⽹络卡顿。


就好像本来在秋名⼭⾼速漂移着，突然来个紧急刹⻋，轮胎受得了吗。。。


##### 发⽣快速重传的拥塞发⽣算法

还有更好的⽅式，前⾯我们讲过「快速重传算法」。当接收⽅发现丢了⼀个中间包的时候，发送三次前⼀个包的ACK，于是发送端就会快速地重传，不必等待超时再重传。

TCP 认为这种情况不严重，因为⼤部分没丢，只丢了⼀⼩部分，则 ssthresh 和 cwnd 变化如下：

- cwnd = cwnd/2 ，也就是设置为原来的⼀半;
- ssthresh = cwnd ;
- 进⼊快速恢复算法



#### 快速恢复
快速重传和快速恢复算法⼀般同时使⽤，快速恢复算法是认为，你还能收到 3 个重复 ACK 说明⽹络也不那么糟糕，所以没有必要像 RTO 超时那么强烈。

正如前⾯所说，进⼊快速恢复之前， cwnd 和 ssthresh 已被更新了：

- cwnd = cwnd/2 ，也就是设置为原来的⼀半;
- ssthresh = cwnd ;


然后，进⼊快速恢复算法如下：

- 拥塞窗⼝ cwnd = ssthresh + 3 （ 3 的意思是确认有 3 个数据包被收到了）；

- 重传丢失的数据包；

- 如果再收到重复的 ACK，那么 cwnd 增加 1；

- 如果收到新数据的 ACK 后，把 cwnd 设置为第⼀步中的 ssthresh 的值，原因是该 ACK 确认了新的数据，说明从 duplicated ACK 时的数据都已收到，该恢复过程已经结束，可以回到恢复之前的状态了，也即再次进⼊拥塞避免状态；


也就是没有像「超时重传」⼀夜回到解放前，⽽是还在⽐较⾼的值，后续呈线性增⻓。


#### 拥塞算法示意图

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211114510.png)



### 3.48 TCP 实战抓包分析

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211115516.png)


⽹络世界中的数据包交互我们⾁眼是看不⻅的，它们就好像隐形了⼀样，我们对着课本学习计算机⽹络的时候就会觉得⾮常的抽象，加⼤了学习的难度。

tcpdump 和 Wireshark，这两⼤利器把我们“看不⻅”的数据包，呈现在我们眼前，⼀⽬了然。


### 3.49 tcpdump 和 Wireshark 有什么区别？

tcpdump 和 Wireshark 就是最常⽤的⽹络抓包和分析⼯具，更是分析⽹络性能必不可少的利器。


- tcpdump 仅⽀持命令⾏格式使⽤，常⽤在 Linux 服务器中抓取和分析⽹络包。

- Wireshark 除了可以抓包外，还提供了可视化分析⽹络包的图形⻚⾯。



所以，这两者实际上是搭配使⽤的，先⽤ tcpdump 命令在 Linux 服务器上抓包，接着把抓包的⽂件拖出到
Windows 电脑后，⽤ Wireshark 可视化分析



当然，如果你是在 Windows 上抓包，只需要⽤ Wireshark ⼯具就可以。



### 3.50 tcpdump 在 Linux 下如何抓包？
tcpdump 提供了⼤量的选项以及各式各样的过滤表达式，来帮助你抓取指定的数据包，不过不要担⼼，只需要掌握⼀些常⽤选项和过滤表达式，就可以满⾜⼤部分场景的需要了。

假设我们要抓取下⾯的 ping 的数据包：


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211121630.png)

要抓取上⾯的 ping 命令数据包，⾸先我们要知道 ping 的数据包是 icmp 协议，接着在使⽤ tcpdump 抓包的时候，就可以指定只抓 icmp 协议的数据包：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211121848.png)


那么当 tcpdump 抓取到 icmp 数据包后， 输出格式如下：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211123657.png)


从 tcpdump 抓取的 icmp 数据包，我们很清楚的看到 icmp echo 的交互过程了，⾸先发送⽅发起了 ICMP echo request 请求报⽂，接收⽅收到后回了⼀个 ICMP echo reply 响应报⽂，之后 seq 是递增的。


我在这⾥也帮你整理了⼀些最常⻅的⽤法，并且绘制成了表格，你可以参考使⽤。

⾸先，先来看看常⽤的选项类，在上⾯的 ping 例⼦中，我们⽤过 -i 选项指定⽹⼝，⽤过 -nn 选项不对 IP 地址和端⼝名称解析。其他常⽤的选项，如下表格：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211144611.png)


接下来，我们再来看看常⽤的过滤表⽤法，在上⾯的 ping 例⼦中，我们⽤过的是 icmp and host
183.232.231.174 ，表示抓取 icmp 协议的数据包，以及源地址或⽬标地址为 183.232.231.174 的包。其他常⽤的过滤选项，我也整理成了下⾯这个表格。


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211145011.png)


tcpdump 虽然功能强⼤，但是输出的格式并不直观。

所以，在⼯作中 tcpdump 只是⽤来抓取数据包，不⽤来分析数据包，⽽是把 tcpdump 抓取的数据包保存成 pcap后缀的⽂件，接着⽤ Wireshark ⼯具进⾏数据包分析。



### 3.51 Wireshark ⼯具如何分析数据包？
Wireshark 除了可以抓包外，还提供了可视化分析⽹络包的图形⻚⾯，同时，还内置了⼀系列的汇总分析⼯具。

⽐如，拿上⾯的 ping 例⼦来说，我们可以使⽤下⾯的命令，把抓取的数据包保存到 ping.pcap ⽂件


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211145807.png)

接着把 ping.pcap ⽂件拖到电脑，再⽤ Wireshark 打开它。打开后，你就可以看到下⾯这个界⾯：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211145947.png)

是吧？在 Wireshark 的⻚⾯⾥，可以更加直观的分析数据包，不仅展示各个⽹络包的头部信息，还会⽤不同的颜⾊来区分不同的协议，由于这次抓包只有 ICMP 协议，所以只有紫⾊的条⽬。


接着，在⽹络包列表中选择某⼀个⽹络包后，在其下⾯的⽹络包详情中，可以更清楚的看到，这个⽹络包在协议栈各层的详细信息。⽐如，以编号 1 的⽹络包为例⼦：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211150257.jpg)


- 可以在数据链路层，看到 MAC 包头信息，如源 MAC 地址和⽬标 MAC 地址等字段

- 可以在 IP 层，看到 IP 包头信息，如源 IP 地址和⽬标 IP 地址、TTL、IP 包⻓度、协议等 IP 协议各个字段的数值和含义；

- 可以在 ICMP 层，看到 ICMP 包头信息，⽐如 Type、Code 等 ICMP 协议各个字段的数值和含义；

Wireshark ⽤了分层的⽅式，展示了各个层的包头信息，把“不可⻅”的数据包，清清楚楚的展示了给我们，还有理由学不好计算机⽹络吗？是不是相⻅恨晚？


从 ping 的例⼦中，我们可以看到⽹络分层就像有序的分⼯，每⼀层都有⾃⼰的责任范围和信息，上层协议完成⼯作后就交给下⼀层，最终形成⼀个完整的⽹络包。

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211150459.png)




### 3.51 解密 TCP 三次握⼿和四次挥⼿

本次例⼦，我们将要访问的 http://192.168.3.200 服务端。在终端⼀⽤ tcpdump 命令抓取数据包：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211151249.png)




接着，在终端⼆执⾏下⾯的 curl 命令：


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211151344.png)


最后，回到终端⼀，按下 Ctrl+C 停⽌ tcpdump，并把得到的 http.pcap 取出到电脑。


使⽤ Wireshark 打开 http.pcap 后，你就可以在 Wireshark 中，看到如下的界⾯：、

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211152736.jpg)

我们都知道 HTTP 是基于 TCP 协议进⾏传输的，那么：


- 最开始的 3 个包就是 TCP 三次握⼿建⽴连接的包

- 中间是 HTTP 请求和响应的包

- ⽽最后的 3 个包则是 TCP 断开连接的挥⼿包



Wireshark 可以⽤时序图的⽅式显示数据包交互的过程，从菜单栏中，点击 统计 (Statistics) -> 流 图 (FlowGraph)，然后，在弹出的界⾯中的「流 类型」选择 「TCP Flows」，你可以更清晰的看到，整个过程中 TCP 流的执⾏过程：



![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211153933.png)



### 3.52 为什么三次握⼿连接过程的 Seq 是 0 ？

实际上是因为 Wireshark ⼯具帮我们做了优化，它默认显示的是序列号 seq 是相对值，⽽不是真实值。

如果你想看到实际的序列号的值，可以右键菜单， 然后找到「协议⾸选项」，接着找到「Relative Seq」后，把它给取消，操作如下


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211154134.jpg)


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211154251.png)



可⻅，客户端和服务端的序列号实际上是不同的，序列号是⼀个随机值。

这其实跟我们书上看到的 TCP 三次握⼿和四次挥⼿很类似，作为对⽐，你通常看到的 TCP 三次握⼿和四次挥⼿的流程，基本是这样的：


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211155850.png)

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211155905.png)



### 3.53 为什么抓到的 TCP 挥⼿是三次，⽽不是书上说的四次？

因为服务器端收到客户端的 FIN 后，服务器端同时也要关闭连接，这样就可以把 ACK 和 FIN 合并到⼀起发
送，节省了⼀个包，变成了“三次挥⼿”。

⽽通常情况下，服务器端收到客户端的 FIN 后，很可能还没发送完数据，所以就会先回复客户端⼀个 ACK
包，稍等⼀会⼉，完成所有数据包的发送后，才会发送 FIN 包，这也就是四次挥⼿了。


如下图，就是四次挥⼿的过程：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211160402.png)

### 3.54 TCP 三次握⼿异常情况实战分析
TCP 三次握⼿的过程相信⼤家都背的滚⽠烂熟，那么你有没有想过这三个异常情况：

- TCP 第⼀次握⼿的 SYN 丢包了，会发⽣了什么？

- TCP 第⼆次握⼿的 SYN、ACK 丢包了，会发⽣什么？

- TCP 第三次握⼿的 ACK 包丢了，会发⽣什么？


有的⼩伙伴可能说：“很简单呀，包丢了就会重传嘛。”

那我在继续问你：

- 那会重传⼏次？

- 超时重传的时间 RTO 会如何变化？

- 在 Linux 下如何设置重传次数？


是不是哑⼝⽆⾔，⽆法回答？

#### 实验场景

本次实验⽤了两台虚拟机，⼀台作为服务端，⼀台作为客户端，它们的关系如下：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211160819.png)


- 客户端和服务端都是 CentOs 6.5 Linux，Linux 内核版本 2.6.32
- 服务端 192.168.12.36，apache web 服务
- 客户端 192.168.12.37

##### 实验⼀：TCP 第⼀次握⼿ SYN 丢包
为了模拟 TCP 第⼀次握⼿ SYN 丢包的情况，我是在拔掉服务器的⽹线后，⽴刻在客户端执⾏ curl 命令：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211161037.png)

其间 tcpdump 抓包的命令如下：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211161132.png)

过了⼀会， curl 返回了超时连接的错误：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211161201.png)


从 date 返回的时间，可以发现在超时接近 1 分钟的时间后，curl 返回了错误。

接着，把 tcp_sys_timeout.pcap ⽂件⽤ Wireshark 打开分析，显示如下图：


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211161424.jpg)


从上图可以发现， 客户端发起了 SYN 包后，⼀直没有收到服务端的 ACK ，所以⼀直超时重传了 5 次，并且每次RTO 超时时间是不同的：

- 第⼀次是在 1 秒超时重传

- 第⼆次是在 3 秒超时重传

- 第三次是在 7 秒超时重传

- 第四次是在 15 秒超时重传

- 第五次是在 31 秒超时重传

可以发现，每次超时时间 RTO 是指数（翻倍）上涨的，当超过最⼤重传次数后，客户端不再发送 SYN 包。

在 Linux 中，第⼀次握⼿的 SYN 超时重传次数，是如下内核参数指定的：


$ cat /proc/sys/net/ipv4/tcp_syn_retries
5

tcp_syn_retries 默认值为 5，也就是 SYN 最⼤重传次数是 5 次。


接下来，我们继续做实验，把 tcp_syn_retries 设置为 2 次：


$ echo 2 > /proc/sys/net/ipv4/tcp_syn_retries


重传抓包后，⽤ Wireshark 打开分析，显示如下图：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211162151.png)


实验⼀的实验⼩结

通过实验⼀的实验结果，我们可以得知，当客户端发起的 TCP 第⼀次握⼿ SYN 包，在超时时间内没收到服务端的ACK，就会在超时重传 SYN 数据包，每次超时重传的 RTO 是翻倍上涨的，直到 SYN 包的重传次数到达tcp_syn_retries 值后，客户端不再发送 SYN 包。

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211162606.png)


##### TCP 第⼆次握⼿ SYN、ACK 丢包
为了模拟客户端收不到服务端第⼆次握⼿ SYN、ACK 包，我的做法是在客户端加上防⽕墙限制，直接粗暴的把来⾃服务端的数据都丢弃，防⽕墙的配置如下：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211163044.png)


接着，在客户端执⾏ curl 命令：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211163109.png)


从 date 返回的时间前后，可以算出⼤概 1 分钟后，curl 报错退出了。

客户端在这其间抓取的数据包，⽤ Wireshark 打开分析，显示的时序图如下：


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211163411.png)


从图中可以发现：
- 客户端发起 SYN 后，由于防⽕墙屏蔽了服务端的所有数据包，所以 curl 是⽆法收到服务端的 SYN、ACK
包，当发⽣超时后，就会重传 SYN 包。

- 服务端收到客户的 SYN 包后，就会回 SYN、ACK 包，但是客户端⼀直没有回 ACK，服务端在超时后，重传了 SYN、ACK 包，接着⼀会，客户端超时重传的 SYN 包⼜抵达了服务端，服务端收到后，超时定时器就重
新计时，然后回了 SYN、ACK 包，所以相当于服务端的超时定时器只触发了⼀次，⼜被重置了。

- 最后，客户端 SYN 超时重传次数达到了 5 次（tcp_syn_retries 默认值 5 次），就不再继续发送 SYN 包了。

所以，我们可以发现，当第⼆次握⼿的 SYN、ACK 丢包时，客户端会超时重发 SYN 包，服务端也会超时重传
SYN、ACK 包。

### 3.55 咦？客户端设置了防⽕墙，屏蔽了服务端的⽹络包，为什么 tcpdump 还能抓到服务端的⽹络包？
添加 iptables 限制后， tcpdump 是否能抓到包 ，这要看添加的 iptables 限制条件：

- 如果添加的是 INPUT 规则，则可以抓得到包
- 如果添加的是 OUTPUT 规则，则抓不到包

⽹络包进⼊主机后的顺序如下：

- 进来的顺序 Wire -> NIC -> tcpdump -> netfilter/iptables

- 出去的顺序 iptables -> tcpdump -> NIC -> Wire


### 3.55 tcp_syn_retries 是限制 SYN 重传次数，那第⼆次握⼿ SYN、ACK 限制最⼤重传次数是多少？

TCP 第⼆次握⼿ SYN、ACK 包的最⼤重传次数是通过 tcp_synack_retries 内核参数限制的，其默认值如下：


$ cat /proc/sys/net/ipv4/tcp_synack_retries
5

是的，TCP 第⼆次握⼿ SYN、ACK 包的最⼤ 传次数默认值是 5 次。


为了验证 SYN、ACK 包最⼤重传次数是 5 次，我们继续做下实验，我们先把客户端的 tcp_syn_retries 设置为1，表示客户端 SYN 最⼤超时次数是 1 次，⽬的是为了防⽌多次重传 SYN，把服务端 SYN、ACK 超时定时器重置。


接着，还是如上⾯的步骤：

- 1. 客户端配置防⽕墙屏蔽服务端的数据包

- 2. 客户端 tcpdump 抓取 curl 执⾏时的数据包


把抓取的数据包，⽤ Wireshark 打开分析，显示的时序图如下：


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211164426.png)


从上图，我们可以分析出：
- 客户端的 SYN 只超时重传了 1 次，因为 tcp_syn_retries 值为 1

- 服务端应答了客户端超时重传的 SYN 包后，由于⼀直收不到客户端的 ACK 包，所以服务端⼀直在超时重传SYN、ACK 包，每次的 RTO 也是指数上涨的，⼀共超时重传了 5 次，因为 tcp_synack_retries 值为 5


接着，我把 tcp_synack_retries 设置为 2， tcp_syn_retries 依然设置为 1:

echo 2 > /proc/sys/net/ipv4/tcp_synack_retries

echo 1 > /proc/sys/net/ipv4/tcp_syn_retries


依然保持⼀样的实验步骤进⾏操作，接着把抓取的数据包，⽤ Wireshark 打开分析，显示的时序图如下：


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211164654.png)



可⻅：

- 客户端的 SYN 包只超时重传了 1 次，符合 tcp_syn_retries 设置的值；

- 服务端的 SYN、ACK 超时重传了 2 次，符合 tcp_synack_retries 设置的值


实验⼆的实验⼩结

通过实验⼆的实验结果，我们可以得知，当 TCP 第⼆次握⼿ SYN、ACK 包丢了后，客户端 SYN 包会发⽣超时重传，服务端 SYN、ACK 也会发⽣超时重传。

客户端 SYN 包超时重传的最⼤次数，是由 tcp_syn_retries 决定的，默认值是 5 次；服务端 SYN、ACK 包时重传的最⼤次数，是由 tcp_synack_retries 决定的，默认值是 5 次。

##### 实验三：TCP 第三次握⼿ ACK 丢包

为了模拟 TCP 第三次握⼿ ACK 包丢，我的实验⽅法是在服务端配置防⽕墙，屏蔽客户端 TCP 报⽂中标志位是ACK 的包，也就是当服务端收到客户端的 TCP ACK 的报⽂时就会丢弃，iptables 配置命令如下：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211165136.png)


接着，在客户端执⾏如下 tcpdump 命令：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211165201.png)

然后，客户端向服务端发起 telnet，因为 telnet 命令是会发起 TCP 连接，所以⽤此命令做测试：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211165227.jpg)

此时，由于服务端收不到第三次握⼿的 ACK 包，所以⼀直处于 SYN_RECV 状态：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211165507.png)

⽽客户端是已完成 TCP 连接建⽴，处于 ESTABLISHED 状态：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211165532.png)

过了 1 分钟后，观察发现服务端的 TCP 连接不⻅了：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211165559.png)

过了 30 分别，客户端依然还是处于 ESTABLISHED 状态：
![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211165625.png)

接着，在刚才客户端建⽴的 telnet 会话，输⼊ 123456 字符，进⾏发送：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211165649.jpg)

持续「好⻓」⼀段时间，客户端的 telnet 才断开连接：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211165716.jpg)


以上就是本次的实现三的现象，这⾥存在两个疑点：

- 为什么服务端原本处于 SYN_RECV 状态的连接，过 1 分钟后就消失了？

- 为什么客户端 telnet 输⼊ 123456 字符后，过了好⻓⼀段时间，telnet 才断开连接？


不着急，我们把刚抓的数据包，⽤ Wireshark 打开分析，显示的时序图如下：


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211165751.png)

上图的流程：
- 客户端发送 SYN 包给服务端，服务端收到后，回了个 SYN、ACK 包给客户端，此时服务端的 TCP 连接处于SYN_RECV 状态；

- 客户端收到服务端的 SYN、ACK 包后，给服务端回了个 ACK 包，此时客户端的 TCP 连接处于ESTABLISHED 状态


- 由于服务端配置了防⽕墙，屏蔽了客户端的 ACK 包，所以服务端⼀直处于 SYN_RECV 状态，没有进⼊ESTABLISHED 状态，tcpdump 之所以能抓到客户端的 ACK 包，是因为数据包进⼊系统的顺序是先进⼊tcpudmp，后经过 iptables；


- 接着，服务端超时 传了 SYN、ACK 包， 传了 5 次后，也就是超过 tcp_synack_retries 的值（默认值是5），然后就没有继续重传了，此时服务端的 TCP 连接主动中⽌了，所以刚才处于 SYN_RECV 状态的 TCP连接断开了，⽽客户端依然处于 ESTABLISHED 状态；


- 虽然服务端 TCP 断开了，但过了⼀段时间，发现客户端依然处于ESTABLISHED 状态，于是就在客户端的telnet 会话输⼊了 123456 字符；




- 此时由于服务端已经断开连接，客户端发送的数据报⽂，⼀直在超时重传，每⼀次重传，RTO 的值是指数增
⻓的，所以持续了好⻓⼀段时间，客户端的 telnet 才报错退出了，此时共重传了 15 次。


通过这⼀波分析，刚才的两个疑点已经解除了：

- 服务端在重传 SYN、ACK 包时，超过了最⼤重传次数 tcp_synack_retries ，于是服务端的 TCP 连接主动断开了。

- 客户端向服务端发送数据包时，由于服务端的 TCP 连接已经退出了，所以数据包⼀直在超时重传，共重传了15 次， telnet 就断开了连接。



### 3.56  TCP 第⼀次握⼿的 SYN 包超时重传最⼤次数是由 tcp_syn_retries 指定，TCP 第⼆次握⼿的 SYN、ACK 包超时重传最⼤次数是由 tcp_synack_retries 指定，那 TCP 建⽴连接后的数据包最⼤超时重传次数是由什么参数指定呢？

TCP 建⽴连接后的数据包传输，最⼤超时重传次数是由 tcp_retries2 指定，默认值是 15 次，如下：

$ cat /proc/sys/net/ipv4/tcp_retries2
15

如果 15 次重传都做完了，TCP 就会告诉应⽤层说：“搞不定了，包怎么都传不过去！”

### 3.57  那如果客户端不发送数据，什么时候才会断开处于 ESTABLISHED 状态的连接？
这⾥就需要提到 TCP 的 保活机制。这个机制的原理是这样的：

定义⼀个时间段，在这个时间段内，如果没有任何连接相关的活动，TCP 保活机制会开始作⽤，每隔⼀个时间间隔，发送⼀个「探测报⽂」，该探测报⽂包含的数据⾮常少，如果连续⼏个探测报⽂都没有得到响应，则认为当前的 TCP 连接已经死亡，系统内核将错误信息通知给上层应⽤程序。

在 Linux 内核可以有对应的参数可以设置保活时间、保活探测的次数、保活探测的时间间隔，以下都为默认值：

net.ipv4.tcp_keepalive_time=7200
net.ipv4.tcp_keepalive_intvl=75 
net.ipv4.tcp_keepalive_probes=9


- tcp_keepalive_time=7200：表示保活时间是 7200 秒（2⼩时），也就 2 ⼩时内如果没有任何连接相关的活动，则会启动保活机制

- tcp_keepalive_intvl=75：表示每次检测间隔 75 秒；

- tcp_keepalive_probes=9：表示检测 9 次⽆响应，认为对⽅是不可达的，从⽽中断本次的连接。


也就是说在 Linux 系统中，最少需要经过 2 ⼩时 11 分 15 秒才可以发现⼀个「死亡」连接。


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211174528.png)


实验三的实验⼩结

在建⽴ TCP 连接时，如果第三次握⼿的 ACK，服务端⽆法收到，则服务端就会短暂处于 SYN_RECV 状态，⽽
客户端会处于 ESTABLISHED 状态。

由于服务端⼀直收不到 TCP 第三次握⼿的 ACK，则会⼀直重传 SYN、ACK 包，直到重传次数超过
tcp_synack_retries 值（默认值 5 次）后，服务端就会断开 TCP 连接。


⽽客户端则会有两种情况：

- 如果客户端没发送数据包，⼀直处于 ESTABLISHED 状态，然后经过 2 ⼩时 11 分 15 秒才可以发现⼀个
「死亡」连接，于是客户端连接就会断开连接。


- 如果客户端发送了数据包，⼀直没有收到服务端对该数据包的确认报⽂，则会⼀直重传该数据包，直到重传
次数超过 tcp_retries2 值（默认值 15 次）后，客户端就会断开 TCP 连接。


### 3.58 TCP 快速建⽴连接

客户端在向服务端发起 HTTP GET 请求时，⼀个完整的交互过程，需要 2.5 个 RTT 的时延。

由于第三次握⼿是可以携带数据的，这时如果在第三次握⼿发起 HTTP GET 请求，需要 2 个 RTT 的时延。

但是在下⼀次（不是同个 TCP 连接的下⼀次）发起 HTTP GET 请求时，经历的 RTT 也是⼀样，如下图：


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211175201.png)


在 Linux 3.7 内核版本中，提供了 TCP Fast Open 功能，这个功能可以减少 TCP 连接建⽴的时延。

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211175355.png)

- 在第⼀次建⽴连接的时候，服务端在第⼆次握⼿产⽣⼀个 Cookie （已加密）并通过 SYN、ACK 包⼀起发
给客户端，于是客户端就会缓存这个 Cookie ，所以第⼀次发起 HTTP Get 请求的时候，还是需要 2 个 RTT
的时延；


- 在下次请求的时候，客户端在 SYN 包带上 Cookie 发给服务端，就提前可以跳过三次握⼿的过程，因为
Cookie 中维护了⼀些信息，服务端可以从 Cookie 获取 TCP 相关的信息，这时发起的 HTTP GET 请求就
只需要 1 个 RTT 的时延；

注：客户端在请求并存储了 Fast Open Cookie 之后，可以不断重复 TCP Fast Open 直⾄服务器认为 Cookie ⽆效（通常为过期）



### 3.59 在 Linux 上如何打开 Fast Open 功能？

可以通过设置 net.ipv4.tcp_fastopn 内核参数，来打开 Fast Open 功能。

net.ipv4.tcp_fastopn 各个值的意义:

- 0 关闭
- 1 作为客户端使⽤ Fast Open 功能
- 2 作为服务端使⽤ Fast Open 功能
- 3 ⽆论作为客户端还是服务器，都可以使⽤ Fast Open 功能


### 3.60 TCP Fast Open 抓包分析
在下图，数据包 7 号，客户端发起了第⼆次 TCP 连接时，SYN 包会携带 Cooike，并且⻓度为 5 的数据。

服务端收到后，校验 Cooike 合法，于是就回了 SYN、ACK 包，并且确认应答收到了客户端的数据包，ACK = 5 +1 = 6

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211175901.jpg)


### 3.61 TCP 重复确认和快速重传

当接收⽅收到乱序数据包时，会发送重复的 ACK，以便告知发送⽅要重发该数据包，当发送⽅收到 3 个重复 ACK时，就会触发快速重传，⽴刻重发丢失数据包。

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211180144.png)


TCP 重复确认和快速重传的⼀个案例，⽤ Wireshark 分析，显示如下：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211180326.jpg)

- 数据包 1 期望的下⼀个数据包 Seq 是 1，但是数据包 2 发送的 Seq 却是 10945，说明收到的是乱序数据包，于是回了数据包 3 ，还是同样的 Seq = 1，Ack = 1，这表明是重复的 ACK；


- 数据包 4 和 6 依然是乱序的数据包，于是依然回了重复的 ACK；

- 当对⽅收到三次重复的 ACK 后，于是就快速重传了 Seq = 1 、Len = 1368 的数据包 8；

- 当收到重传的数据包后，发现 Seq = 1 是期望的数据包，于是就发送了个确认收到快速重传的 ACK


注意：快速 传和 复 ACK 标记信息是 Wireshark 的功能，⾮数据包本身的信息。


以上案例在 TCP 三次握⼿时协商开启了选择性确认 SACK，因此⼀旦数据包丢失并收到重复 ACK ，即使在丢失数据包之后还成功接收了其他数据包，也只需要重传丢失的数据包。如果不启⽤ SACK，就必须重传丢失包之后的每个数据包。


如果要⽀持 SACK ，必须双⽅都要⽀持。在 Linux 下，可以通过 net.ipv4.tcp_sack 参数打开这个功能（Linux2.4 后默认打开）。



### 3.62 TCP 流量控制

TCP 为了防⽌发送⽅⽆脑的发送数据，导致接收⽅缓冲区被填满，所以就有了滑动窗⼝的机制，它可利⽤接收⽅的接收窗⼝来控制发送⽅要发送的数据 ，也就是流量控制。

接收窗⼝是由接收⽅指定的值，存储在 TCP 头部中，它可以告诉发送⽅⾃⼰的 TCP 缓冲空间区⼤⼩，这个缓冲区是给应⽤程序读取数据的空间：

- 如果应⽤程序读取了缓冲区的数据，那么缓冲空间区就会把被读取的数据移除

- 如果应⽤程序没有读取数据，则数据会⼀直滞留在缓冲区。

接收窗⼝的⼤⼩，是在 TCP 三次握⼿中协商好的，后续数据传输时，接收⽅发送确认应答 ACK 报⽂时，会携带当前的接收窗⼝的⼤⼩，以此来告知发送⽅。


假设接收⽅接收到数据后，应⽤层能很快的从缓冲区⾥读取数据，那么窗⼝⼤⼩会⼀直保持不变，过程如下：

但是现实中服务器会出现繁忙的情况，当应⽤程序读取速度慢，那么缓存空间会慢慢被占满，于是为了保证发送⽅发送的数据不会超过缓冲区⼤⼩，服务器则会调整窗⼝⼤⼩的值，接着通过 ACK 报⽂通知给对⽅，告知现在的接收窗⼝⼤⼩，从⽽控制发送⽅发送的数据⼤⼩。

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211183107.png)


#### 零窗⼝通知与窗⼝探测
假设接收⽅处理数据的速度跟不上接收数据的速度，缓存就会被占满，从⽽导致接收窗⼝为 0，当发送⽅接收到零窗⼝通知时，就会停⽌发送数据。

如下图，可以看到接收⽅的窗⼝⼤⼩在不断的收缩⾄ 0：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211183358.png)



接着，发送⽅会**定时发送窗⼝⼤⼩探测报⽂**，以便及时知道接收⽅窗⼝⼤⼩的变化。


以下图 Wireshark 分析图作为例⼦说明：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211183512.png)

- 发送⽅发送了数据包 1 给接收⽅，接收⽅收到后，由于缓冲区被占满，回了个零窗⼝通知；

- 发送⽅收到零窗⼝通知后，就不再发送数据了，直到过了 3.4 秒后，发送了⼀个 TCP Keep-Alive 报⽂，也就是窗⼝⼤⼩探测报⽂；

- 当接收⽅收到窗⼝探测报⽂后，就⽴⻢回⼀个窗⼝通知，但是窗⼝⼤⼩还是 0；

- 发送⽅发现窗⼝还是 0，于是继续等待了 6.8 （翻倍） 秒后，⼜发送了窗⼝探测报⽂，接收⽅依然还是回了窗⼝为 0 的通知；

- 发送⽅发现窗⼝还是 0，于是继续等待了 13.5 （翻倍） 秒后，⼜发送了窗⼝探测报⽂，接收⽅依然还是回了窗⼝为 0 的通知；

可以发现，这些窗⼝探测报⽂以 3.4s、6.5s、13.5s 的间隔出现，说明超时时间会翻倍递增。

这连接暂停了 25s，想象⼀下你在打王者的时候，25s 的延迟你还能上王者吗？


### 3.63  在 Wireshark 看到的 Windows size 也就是 " win = "，这个值表示发送窗⼝吗？

这不是发送窗⼝，⽽是在向对⽅声明⾃⼰的**接收窗⼝。**


你可能会好奇，抓包⽂件⾥有「Window size scaling factor」，它其实是算出实际窗⼝⼤⼩的乘法因⼦，
「Window size value」实际上并不是真实的窗⼝⼤⼩，真实窗⼝⼤⼩的计算公式如下：

「Window size value」 * 「Window size scaling factor」 = 「Caculated window size 」


对应的下图案例，也就是 32 * 2048 = 65536。

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211184307.jpg)

实际上是 Caculated window size 的值是 Wireshark ⼯具帮我们算好的，Window size scaling factor 和 Windossize value 的值是在 TCP 头部中，其中 Window size scaling factor 是在三次握⼿过程中确定的，如果你抓包的数据没有 TCP 三次握⼿，那可能就⽆法算出真实的窗⼝⼤⼩的值，如下图：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211184440.jpg)



### 3.64 如何在包⾥看出发送窗⼝的⼤⼩？

很遗憾，没有简单的办法，发送窗⼝虽然是由接收窗⼝决定，但是它⼜可以被⽹络因素影响，也就是拥塞窗⼝，实际上发送窗⼝是值是 min(拥塞窗⼝，接收窗⼝)。

### 3.65 发送窗⼝和 MSS 有什么关系？
发送窗⼝决定了⼀⼝⽓能发多少字节，⽽ MSS 决定了这些字节要分多少包才能发完。

举个例⼦，如果发送窗⼝为 16000 字节的情况下，如果 MSS 是 1000 字节，那就需要发送 1600/1000 = 16 个包。

### 3.66 发送⽅在⼀个窗⼝发出 n 个包，是不是需要 n 个 ACK 确认报⽂？
不⼀定，因为 TCP 有累计确认机制，所以当收到多个数据包时，只需要应答最后⼀个数据包的 ACK 报⽂就可以了。


### 3.67 TCP 延迟确认与 Nagle 算法

当我们 TCP 报⽂的承载的数据⾮常⼩的时候，例如⼏个字节，那么整个⽹络的效率是很低的，因为每个 TCP 报⽂中都会有 20 个字节的 TCP 头部，也会有 20 个字节的 IP 头部，⽽数据只有⼏个字节，所以在整个报⽂中有效数据占有的⽐ 就会⾮常低。

这就好像快递员开着⼤货⻋送⼀个⼩包裹⼀样浪费。

那么就出现了常⻅的两种策略，来减少⼩报⽂的传输，分别是

- Nagle 算法

- 延迟确认


### 3.68 Nagle 算法是如何避免⼤量 TCP ⼩数据报⽂的传输？

Nagle 算法做了⼀些策略来避免过多的⼩数据报⽂发送，这可提⾼传输效率。

Nagle 算法的策略：

- 没有已发送未确认报⽂时，⽴刻发送数据。

- 存在未确认报⽂时，直到「没有已发送未确认报⽂」或「数据⻓度达到 MSS ⼤⼩」时，再发送数据。

只要没满⾜上⾯条件中的⼀条，发送⽅⼀直在囤积数据，直到满⾜上⾯的发送条件。


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211205620.png)


上图右侧启⽤了 Nagle 算法，它的发送数据的过程：

- ⼀开始由于没有已发送未确认的报⽂，所以就⽴刻发了 H 字符；

- 接着，在还没收到对 H 字符的确认报⽂时，发送⽅就⼀直在囤积数据，直到收到了确认报⽂后，此时没有已发送未确认的报⽂，于是就把囤积后的 ELL 字符⼀起发给了接收⽅；

- 待收到对 ELL 字符的确认报⽂后，于是把最后⼀个 O 字符发送了出去

可以看出，Nagle 算法⼀定会有⼀个⼩报⽂，也就是在最开始的时候。

另外，Nagle 算法默认是打开的，如果对于⼀些需要⼩数据包交互的场景的程序，⽐如，telnet 或 ssh 这样的交互性⽐较强的程序，则需要关闭 Nagle 算法。

可以在 Socket 设置 TCP_NODELAY 选项来关闭这个算法（关闭 Nagle 算法没有全局参数，需要根据每个应⽤⾃⼰的特点来关闭）。





### 3.69 那延迟确认⼜是什么？

事实上当没有携带数据的 ACK，它的⽹络效率也是很低的，因为它也有 40 个字节的 IP 头 和 TCP 头，但却没有携带数据报⽂。

为了解决 ACK 传输效率低问题，所以就衍⽣出了 TCP 延迟确认。

TCP 延迟确认的策略：

- 当有响应数据要发送时，ACK 会随着响应数据⼀起⽴刻发送给对⽅

- 当没有响应数据要发送时，ACK 将会延迟⼀段时间，以等待是否有响应数据可以⼀起发送

- 如果在延迟等待发送 ACK 期间，对⽅的第⼆个数据报⽂⼜到达了，这时就会⽴刻发送 ACK



![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211211507.png)



延迟等待的时间是在 Linux 内核中定义的，如下图：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211211621.png)



知道了 HZ 的⼤⼩，那么就可以算出：

- 最⼤延迟确认时间是 200 ms （1000/5）

- 最短延迟确认时间是 40 ms （1000/25）


TCP 延迟确认可以在 Socket 设置 TCP_QUICKACK 选项来关闭这个算法。

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211211806.png)

### 3.70 延迟确认 和 Nagle 算法混合使⽤时，会产⽣新的问题


当 TCP 延迟确认 和 Nagle 算法混合使⽤时，会导致时耗增⻓，如下图：



![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211211211927.png)


发送⽅使⽤了 Nagle 算法，接收⽅使⽤了 TCP 延迟确认会发⽣如下的过程：

- 发送⽅先发出⼀个⼩报⽂，接收⽅收到后，由于延迟确认机制，⾃⼰⼜没有要发送的数据，只能⼲等着发送
⽅的下⼀个报⽂到达；

- ⽽发送⽅由于 Nagle 算法机制，在未收到第⼀个报⽂的确认前，是不会发送后续的数据；

- 所以接收⽅只能等待最⼤时间 200 ms 后，才回 ACK 报⽂，发送⽅收到第⼀个报⽂的确认报⽂后，也才可以发送后续的数据。

很明显，这两个同时使⽤会造成额外的时延，这就会使得⽹络"很慢"的感觉。

要解决这个问题，只有两个办法：

- 要不发送⽅关闭 Nagle 算法

- 要不接收⽅关闭 TCP 延迟确认


### 3.70 tcp_retries1 参数，是什么场景下⽣效？tcp_retries2是不是只受限于规定的次数，还是受限于次数和时间限制的最⼩值？”

tcp_retries1和tcp_retries2都是在TCP三次握⼿之后的场景。

- 当重传次数超过tcp_retries1就会指示 IP 层进⾏ MTU 探测、刷新路由等过程，并不会断开TCP连接，当 传次数超过 tcp_retries2 才会断开TCP流


- tcp_retries1 和 tcp_retries2 两个重传次数都是受⼀个 timeout 值限制的，timeout 的值是根据它俩的值计算出来的，当重传时间超过 timeout，就不会继续重传了，即使次数还没到达。

### 3.71 tcp_orphan_retries也是控制tcp连接的关闭。这个跟tcp_retries1 tcp_retries2有什么区别吗？
主动⽅发送 FIN 报⽂后，连接就处于 FIN_WAIT1 状态下，该状态通常应在数⼗毫秒内转为 FIN_WAIT2，如果迟迟收不到对方返回的ACK时，此时，内核会定时重发FIN报文，其中重发次数由tcp_orphan_retries参数控制。

### 3.72 请问，为什么连续两个报⽂的seq会是⼀样的呢，⽐如三次握⼿之后的那个报⽂？还是说，序号相同的是同⼀个报⽂，只是拆开显示了？

- 1. 三次握⼿中的前两次，是 seq+1；

- 2. 三次握⼿中的最后⼀个 ack，实际上是可以携带数据的，由于我⽂章的例⼦是没有发送数据的，你可以看到第三次握⼿的 len=0 ，在数据传输阶段「下⼀个 seq=seq+len 」，所以第三次握⼿的 seq 和下⼀个数据报的seq 是⼀样的，因为 len 为 0；


### 3.73 TCP 半连接队列和全连接队列
⽹上许多博客针对增⼤ TCP 半连接队列和全连接队列的⽅式如下：

- 增⼤ TCP 半连接队列的⽅式是增⼤ /proc/sys/net/ipv4/tcp_max_syn_backlog；

- 增⼤ TCP 全连接队列的⽅式是增⼤ listen() 函数中的 backlog；


这⾥先跟⼤家说下，上⾯的⽅式都是不准确的，因为我做了实验和看了 TCP 协议栈的内核源码，发现要增⼤这两个队列⻓度，不是简简单单增⼤某⼀个参数就可以的。

接下来，就会以实战 + 源码分析，带⼤家解密 TCP 半连接队列和全连接队列。


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211212093932.png)



### 3.74 在 TCP 三次握⼿的时候，Linux 内核会维护两个队列，分别是：

- 半连接队列，也称 SYN 队列

- 全连接队列，也称 accepet 队列；

服务端收到客户端发起的 SYN 请求后，**内核会把该连接存储到半连接队列**，并向客户端响应SYN+ACK，接着客户端会返回ACK，服务端收到第三次握手的ACK后，**内核会把连接从半连接队列移除，然后创建新的完全的连接，并将其添加到accept队列，等待进程调用accept函数时把连接取出来。**



![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211212095723.png)


不管是半连接队列还是全连接队列，都有最⼤⻓度限制，超过限制时，内核会直接丢弃，或返回 RST 包。


### 3.75 实战 - TCP 全连接队列溢出之如何知道应⽤程序的 TCP 全连接队列⼤⼩？

在服务端可以使⽤ ss 命令，来查看 TCP 全连接队列的情况：

但需要注意的是 ss 命令获取的 Recv-Q/Send-Q 在「LISTEN 状态」和「⾮ LISTEN 状态」所表达的含义是不同的。从下⾯的内核代码可以看出区别：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211212100949.png)

在「LISTEN 状态」时， Recv-Q/Send-Q 表示的含义如下：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211212101922.png)


- Recv-Q：当前全连接队列的⼤⼩，也就是当前已完成三次握⼿并等待服务端 accept() 的 TCP 连接；

- Send-Q：当前全连接最⼤队列⻓度，上⾯的输出结果说明监听 8088 端⼝的 TCP 服务，最⼤全连接⻓度为
128；


在「⾮ LISTEN 状态」时， Recv-Q/Send-Q 表示的含义如下：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211212101926.png)


- Recv-Q：已收到但未被应⽤进程读取的字节数；

- Send-Q：已发送但未收到确认的字节数；

### 3.76 如何模拟 TCP 全连接队列溢出的场景？

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211212101929.png)


实验环境：

- 客户端和服务端都是 CentOs 6.5 ，Linux 内核版本 2.6.32

- 服务端 IP 192.168.3.200，客户端 IP 192.168.3.100

- 服务端是 Nginx 服务，端⼝为 8088

这⾥先介绍下 **wrk** ⼯具，它是⼀款简单的 HTTP 压测⼯具，它能够在单机多核 CPU 的条件下，使⽤系统⾃带的⾼性能 I/O 机制，通过多线程和事件模式，对⽬标机器产⽣⼤量的负载。


本次模拟实验就使⽤ wrk ⼯具来压⼒测试服务端，发起⼤量的请求，⼀起看看服务端 TCP 全连接队列满了会发⽣什么？有什么观察指标？


客户端执⾏ wrk 命令对服务端发起压⼒测试，并发 3 万个连接：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211212104954.png)

在服务端可以使⽤ ss 命令，来查看当前 TCP 全连接队列的情况：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211212105113.png)


其间共执⾏了两次 ss 命令，从上⾯的输出结果，可以发现当前 TCP 全连接队列上升到了 129 ⼤⼩，超过了最⼤TCP 全连接队列。


**当超过了 TCP 最⼤全连接队列，服务端则会丢掉后续进来的 TCP 连接**，丢掉的 TCP 连接的个数会被统计起来，我们可以使⽤ netstat -s 命令来查看：


上⾯看到的 41150 times ，表示全连接队列溢出的次数，注意这个是累计值。可以隔⼏秒钟执⾏下，如果这个数字⼀直在增加的话肯定全连接队列偶尔满了。

从上⾯的模拟结果，可以得知，当服务端并发处理⼤量请求时，如果 TCP 全连接队列过⼩，就容易溢出。发⽣TCP 全连接队溢出的时候，后续的请求就会被丢弃，这样就会出现服务端请求数量上不去的现象。\

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211212131212.png)



### 3.77 Linux 有个参数可以指定当 TCP 全连接队列满了会使⽤什么策略来回应客户端。
实际上，丢弃连接只是 Linux 的默认⾏为，我们还可以选择向客户端发送 RST 复位报⽂，告诉客户端连接已经建⽴失败。

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211212131504.png)


tcp_abort_on_overflow 共有两个值分别是 0 和 1，其分别表示：

- 0 ：如果全连接队列满了，那么 server 扔掉 client 发过来的 ack ；


- 1 ：如果全连接队列满了，server 发送⼀个 reset 包给 client，表示废掉这个握⼿过程和这个连接；



如果要想知道客户端连接不上服务端，是不是服务端 TCP 全连接队列满的原因，那么可以把
tcp_abort_on_overflow 设置为 1，这时如果在客户端异常中可以看到很多 **connection reset by peer** 的错误，那么就可以证明是由于服务端 TCP 全连接队列溢出的问题。


通常情况下，应当把 tcp_abort_on_overflow 设置为 0，因为这样更有利于应对突发流量。


举个例⼦，当 TCP 全连接队列满导致服务器丢掉了 ACK，与此同时，客户端的连接状态却是 ESTABLISHED，进程就在建⽴好的连接上发送请求。只要服务器没有为请求回复 ACK，请求就会被多次重发。如果服务器上的进程只是短暂的繁忙造成 accept 队列满，那么当 TCP 全连接队列有空位时，再次接收到的请求报⽂由于含有 ACK，仍然会触发服务器端成功建⽴连接。


所以，tcp_abort_on_overflow 设为 0 可以提⾼连接建⽴的成功率，只有你⾮常肯定 TCP 全连接队列会⻓期溢出时，才能设置为 1 以尽快通知客户端。

### 3.78 如何增⼤ TCP 全连接队列呢？

是的，当发现 TCP 全连接队列发⽣溢出的时候，我们就需要增⼤该队列的⼤⼩，以便可以应对客户端⼤ 的请求。


**TCP 全连接队列的最⼤值取决于 somaxconn 和 backlog 之间的最⼩值**，也就是 min(somaxconn, backlog)。从下⾯的 Linux 内核代码可以得知：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211212135419.png)


- somaxconn 是 Linux 内核的参数，默认值是 128,可以通过 /proc/sys/net/core/somaxconn 来设置其值；


- backlog 是 listen(int sockfd, int backlog) 函数中的 backlog ⼤⼩，Nginx 默认值是 511，可以通过修改配置⽂件设置其⻓度；

前⾯模拟测试中，我的测试环境：

- somaxconn 是默认值 128；

- Nginx 的 backlog 是默认值 511

所以测试环境的 TCP 全连接队列最⼤值为 min(128, 511)，也就是 128 ，可以执⾏ ss 命令查看：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211212161853.png)

现在我们重新压测，把 TCP 全连接队列搞⼤，把 somaxconn 设置成 5000：


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211212162123.png)


接着把 Nginx 的 backlog 也同样设置成 5000：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211212162305.png)


最后要重启 Nginx 服务，因为只有重新调⽤ listen() 函数 TCP 全连接队列才会重新初始化。

重启完后 Nginx 服务后，服务端执⾏ ss 命令，查看 TCP 全连接队列⼤⼩：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211212162500.png)


从执⾏结果，可以发现 TCP 全连接最⼤值为 5000。


### 3.79 增⼤ TCP 全连接队列后，继续压测

客户端同样以 3 万个连接并发发送请求给服务端：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211212162735.png)



服务端执⾏ ss 命令，查看 TCP 全连接队列使⽤情况：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211212163314.png)


从上⾯的执⾏结果，可以发现全连接队列使⽤增⻓的很快，但是⼀直都没有超过最⼤值，所以就不会溢出，那么netstat -s 就不会有 TCP 全连接队列溢出个数的显示：


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211212164011.png)


说明 TCP 全连接队列最⼤值从 128 增⼤到 5000 后，服务端抗住了 3 万连接并发请求，也没有发⽣全连接队列溢出的现象了。

**如果持续不断地有连接因为 TCP 全连接队列溢出被丢弃，就应该调⼤ backlog 以及 somaxconn 参数。**


### 3.80 实战 - TCP 半连接队列溢出之如何查看 TCP 半连接队列⻓度？

很遗憾，TCP 半连接队列⻓度的⻓度，没有像全连接队列那样可以⽤ ss 命令查看。

但是我们可以抓住 TCP 半连接的特点，**就是服务端处于 SYN_RECV 状态的 TCP 连接，就是 TCP 半连接队列。**

于是，我们可以使⽤如下命令计算当前 TCP 半连接队列⻓度：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211212165257.png)

### 3.81 如何模拟 TCP 半连接队列溢出场景？

模拟 TCP 半连接溢出场景不难，实际上就是对服务端⼀直发送 TCP SYN 包，但是不回第三次握⼿ ACK，这样就会使得服务端有⼤量的处于 SYN_RECV 状态的 TCP 连接。


这其实也就是所谓的 SYN 洪泛、SYN 攻击、DDos 攻击。


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211212165520.png)



实验环境：

客户端和服务端都是 CentOs 6.5 ，Linux 内核版本 2.6.32

服务端 IP 192.168.3.200，客户端 IP 192.168.3.100

服务端是 Nginx 服务，端⼝为 8088

注意：本次模拟实验是没有开启 tcp_syncookies，关于 tcp_syncookies 的作⽤，后续会说明。

本次实验使⽤ **hping3** ⼯具模拟 SYN 攻击：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211212165823.png)


当服务端受到 SYN 攻击后，连接服务端 ssh 就会断开了，⽆法再连上。只能在服务端主机上执⾏查看当前 TCP 半连接队列⼤⼩：


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211212170037.png)


同时，还可以通过 netstat -s 观察半连接队列溢出的情况：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211212171613.png)

上⾯输出的数值是**累计值**，表示共有多少个 TCP 连接因为半连接队列溢出⽽被丢弃。**隔⼏秒执⾏⼏次，如果有上升的趋势，说明当前存在半连接队列溢出的现象。**

### 3.82 ⼤部分⼈都说 tcp_max_syn_backlog 是指定半连接队列的⼤⼩，是真的吗？

很遗憾，半连接队列的⼤⼩并不单单只跟 tcp_max_syn_backlog 有关系。

上⾯模拟 SYN 攻击场景时，服务端的 tcp_max_syn_backlog 的默认值如下：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211212172031.png)


但是在测试的时候发现，服务端最多只有 256 个半连接队列，⽽不是 512，所以**半连接队列的最⼤⻓度不⼀定由tcp_max_syn_backlog 值决定的。**

接下来，⾛进 Linux 内核的源码，来分析 TCP 半连接队列的最⼤值是如何决定的。

TCP 第⼀次握⼿（收到 SYN 包）的 Linux 内核代码如下，其中缩减了⼤量的代码，只需要重点关注 TCP 半连接队列溢出的处理逻辑：


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211212172930.png)


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211212172933.png)

从源码中，我可以得出共有三个条件因队列⻓度的关系⽽被丢弃的：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211212173342.png)


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211212173345.png)


1. 如果半连接队列满了，并且没有开启 tcp_syncookies，则会丢弃；

2. 若全连接队列满了，且没有重传 SYN+ACK 包的连接请求多于 1 个，则会丢弃；

3. 如果没有开启 tcp_syncookies，并且 max_syn_backlog 减去 当前半连接队列⻓度⼩于(max_syn_backlog >> 2)，则会丢弃；


关于 tcp_syncookies 的设置，后⾯在详细说明，可以先给⼤家说⼀下，开启 tcp_syncookies 是缓解 SYN 攻击其中⼀个⼿段。

接下来，我们继续跟⼀下检测半连接队列是否满的函数inet_csk_reqsk_queue_is_full 和 检测全连接队列是否满的函数sk_acceptq_is_full ：



![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211213100233.png)

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211213100423.png)


从上⾯源码，可以得知：

全连接队列的最⼤值是 sk_max_ack_backlog 变量，sk_max_ack_backlog 实际上是在 listen() 源码⾥指定
的，也就是 min(somaxconn, backlog)；


半连接队列的最⼤值是 max_qlen_log 变量，max_qlen_log 是在哪指定的呢？现在暂时还不知道，我们继
续跟进；


我们继续跟进代码，看⼀下是哪⾥初始化了半连接队列的最⼤值 max_qlen_log：




![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211213101502.png)


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211213101505.png)


从上⾯的代码中，我们可以算出 max_qlen_log 是 8，于是代⼊到 检测半连接队列是否满的函数
reqsk_queue_is_full ：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211213101717.png)


也就是 qlen >> 8 什么时候为 1 就代表半连接队列满了。这计算这不难，很明显是当 qlen 为 256 时， 256 >> 8
= 1

⾄此，总算知道为什么上⾯模拟测试 SYN 攻击的时候，服务端处于 SYN_RECV 连接最⼤只有 256 个。



可⻅，**半连接队列最⼤值不是单单由 max_syn_backlog 决定，还跟 somaxconn 和 backlog 有关系。**


在 Linux 2.6.32 内核版本，它们之间的关系，总体可以概况为：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211213102525.png)


### 3.83 半连接队列最⼤值 max_qlen_log 就表示服务端处于 SYN_REVC 状态的最⼤个数吗？

max_qlen_log 是理论半连接队列最⼤值，并不⼀定代表服务端处于 SYN_REVC 状态的最⼤个数。

在前⾯我们在分析 TCP 第⼀次握⼿（收到 SYN 包）时会被丢弃的三种条件：

1. 如果半连接队列满了，并且没有开启 tcp_syncookies，则会丢弃；

2. 若全连接队列满了，且没有重传 SYN+ACK 包的连接请求多于 1 个，则会丢弃；

3. 如果没有开启 tcp_syncookies，并且 max_syn_backlog 减去 当前半连接队列⻓度⼩于(max_syn_backlog >> 2)，则会丢弃；



假设条件 1 当前半连接队列的⻓度 「没有超过」理论的半连接队列最⼤值 max_qlen_log，那么如果条件 3 成⽴，
则依然会丢弃 SYN 包，也就会使得服务端处于 SYN_REVC 状态的最⼤个数不会是理论值 max_qlen_log。

似乎很难理解，我们继续接着做实验，实验⻅真知。


服务端环境如下：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211213104735.png)


配置完后，服务端要重启 Nginx，因为全连接队列最⼤值和半连接队列最⼤值是在 listen() 函数初始化。


根据前⾯的源码分析，我们可以计算出半连接队列 max_qlen_log 的最⼤值为 256：


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211213105124.png)

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211213105126.png)



可以发现，服务端处于 SYN_RECV 状态的最⼤个数并不是 max_qlen_log 变量的值。


我们来分析⼀波条件 3 :

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211213105454.png)


从上⾯的分析，可以得知如果触发「当前半连接队列⻓度 > 192」条件，TCP 第⼀次握⼿的 SYN 包是会被丢弃
的。

在前⾯我们测试的结果，服务端处于 SYN_RECV 状态的最⼤个数是 193，正好是触发了条件 3，所以处于
SYN_RECV 状态的个数还没到「理论半连接队列最⼤值 256」，就已经把 SYN 包丢弃了。

所以，服务端处于 SYN_RECV 状态的最⼤个数分为如下两种情况：


- 如果「当前半连接队列」没超过「理论半连接队列最⼤值」，但是超过 max_syn_backlog -(max_syn_backlog >> 2)，那么处于 SYN_RECV 状态的最⼤个数就是 max_syn_backlog -(max_syn_backlog >> 2)；


- 如果「当前半连接队列」超过「理论半连接队列最⼤值」，那么处于 SYN_RECV 状态的最⼤个数就是「理论半连接队列最⼤值」；



### 3.84 每个 Linux 内核版本「理论」半连接最⼤值计算⽅式会不同。

在上⾯我们是针对 Linux 2.6.32 版本分析的「理论」半连接最⼤值的算法，可能每个版本有些不同。

⽐如在 Linux 5.0.0 的时候，「理论」半连接最⼤值就是全连接队列最⼤值，但依然还是有队列溢出的三个条件：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211213131029.png)


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211213131033.png)


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211213131037.png)

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211213131040.png)

### 3.85 如果 SYN 半连接队列已满，只能丢弃连接吗？

并不是这样，**开启 syncookies 功能就可以在不使⽤ SYN 半连接队列的情况下成功建⽴连接**，在前⾯我们源码分析也可以看到这点，当开启了 syncookies 功能就不会丢弃连接。

syncookies 是这么做的：服务器根据当前状态计算出⼀个值，放在⼰⽅发出的 SYN+ACK 报⽂中发出，当客户端返回 ACK 报⽂时，取出该值验证，如果合法，就认为连接建⽴成功，如下图所示。

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211213144746.png)


syncookies 参数主要有以下三个值：

- 0 值，表示关闭该功能；

- 1 值，表示仅当 SYN 半连接队列放不下时，再启⽤它；

- 2 值，表示⽆条件开启功能；

那么在应对 SYN 攻击时，只需要设置为 1 即可：




![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211213150059.png)


### 3.86 如何防御 SYN 攻击？

这⾥给出⼏种防御 SYN 攻击的⽅法：

- 增⼤半连接队列；

- 开启 tcp_syncookies 功能

- 减少 SYN+ACK 重传次数


#### ⽅式⼀：增⼤半连接队列

在前⾯源码和实验中，得知要想增⼤半连接队列，我们得知不能只单纯增⼤ tcp_max_syn_backlog 的值，还需⼀同增⼤ somaxconn 和 backlog，也就是增⼤全连接队列。否则，只单纯增⼤ tcp_max_syn_backlog 是⽆效的。


增⼤ tcp_max_syn_backlog 和 somaxconn 的⽅法是修改 Linux 内核参数：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211213161305.png)


增⼤ backlog 的⽅式，每个 Web 服务都不同，⽐如 Nginx 增⼤ backlog 的⽅法如下：


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211213161418.png)


最后，改变了如上这些参数后，要重启 Nginx 服务，因为半连接队列和全连接队列都是在 listen() 初始化的。



#### ⽅式⼆：开启 tcp_syncookies 功能

开启 tcp_syncookies 功能的⽅式也很简单，修改 Linux 内核参数：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211213161948.png)


#### ⽅式三：减少 SYN+ACK  传次数

当服务端受到 SYN 攻击时，就会有⼤量处于 SYN_REVC 状态的 TCP 连接，处于这个状态的 TCP 会重传
SYN+ACK ，当重传超过次数达到上限后，就会断开连接。


那么针对 SYN 攻击的场景，我们可以减少 SYN+ACK 的重传次数，以加快处于 SYN_REVC 状态的 TCP 连接断开。


### 3.87 读者问：“syncookies 启⽤后就不需要半链接了？那请求的数据会存在哪⾥？”

syncookies = 1 时，半连接队列满后，后续的请求就不会存放到半连接队列了，⽽是在第⼆次握⼿的时候，服务端会计算⼀个 cookie 值，放⼊到 SYN +ACK 包中的序列号发给客户端，客户端收到后并回 ack ，服务端就会校验连接是否合法，合法就直接把连接放⼊到全连接队列。


### 3.88 TCP 内核参数

TCP 协议是由操作系统实现，所以操作系统提供了不少调节 TCP 的参数。

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211213163514.jpg)


如何正确有效的使⽤这些参数，来提⾼ TCP 性能是⼀个不那么简单事情。我们需要针对 TCP 每个阶段的问题来对症下药，⽽不是病急乱投医。


接下来，将以三个⻆度来阐述提升 TCP 的策略，分别是：

- TCP 三次握⼿的性能提升；

- TCP 四次挥⼿的性能提升；

- TCP 数据传输的性能提升；


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211213164815.png)


### 3.89 TCP 三次握⼿的性能提升

TCP 是⾯向连接的、可靠的、双向传输的传输层通信协议，所以在传输数据之前需要经过三次握⼿才能建⽴连接。


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211213165534.png)


那么，三次握⼿的过程在⼀个 HTTP 请求的平均时间占⽐ 10% 以上，在⽹络状态不佳、⾼并发或者遭遇 SYN 攻击等场景中，如果不能有效正确的调节三次握⼿中的参数，就会对性能产⽣很多的影响。


如何正确有效的使⽤这些参数，来提⾼ TCP 三次握⼿的性能，这就需要理解「三次握⼿的状态变迁」，这样当出现问题时，先⽤ netstat 命令查看是哪个握⼿阶段出现了问题，再来对症下药，⽽不是病急乱投医。

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211213165837.png)


客户端和服务端都可以针对三次握⼿优化性能。主动发起连接的客户端优化相对简单些，⽽服务端需要监听端⼝，属于被动连接⽅，其间保持许多的中间状态，优化⽅法相对复杂⼀些。

所以，客户端（主动发起连接⽅）和服务端（被动连接⽅）优化的⽅式是不同的，接下来分别针对客户端和服务端优化。



#### 客户端优化

三次握⼿建⽴连接的⾸要⽬的是 **「同步序列号」**。

只有同步了序列号才有可靠传输，TCP许多特性都依赖于序列号实现，比如流量控制，丢包重传等，这也是三次握手中的报文称为SYN的原因，SYN的全称就叫Synchronize Sequence Numbers（同步序列号）。

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211214161828.png)



##### SYN_SENT 状态的优化
客户端作为主动发起连接⽅，⾸先它将发送 SYN 包，于是客户端的连接就会处于 SYN_SENT 状态。

客户端在等待服务端回复的 ACK 报⽂，正常情况下，服务器会在⼏毫秒内返回 SYN+ACK ，但如果客户端⻓时间没有收到 SYN+ACK 报⽂，则会重发 SYN 包，重发的次数由 tcp_syn_retries 参数控制，默认是 5 次：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211214163506.png)


通常，第⼀次超时重传是在 1 秒后，第⼆次超时重传是在 2 秒，第三次超时重传是在 4 秒后，第四次超时 重传是在 8 秒后，第五次是在超时重传 16 秒后。没错，每次超时的时间是上⼀次的 2 倍。

当第五次超时重传后，会继续等待 32 秒，如果服务端仍然没有回应 ACK，客户端就会终⽌三次握⼿。

所以，总耗时是 1+2+4+8+16+32=63 秒，⼤约 1 分钟左右。


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211214164752.png)

你可以根据 **⽹络的稳定性**和 **⽬标服务器的繁忙程度**修改 SYN 的重传次数，调整客户端的三次握⼿时间上限。⽐如内⽹中通讯时，就可以适当调低重试次数，尽快把错误暴露给应⽤程序。



#### 服务端优化

当服务端收到 SYN 包后，服务端会⽴⻢回复 SYN+ACK 包，表明确认收到了客户端的序列号，同时也把⾃⼰的序列号发给对⽅。


此时，服务端出现了新连接，状态是 SYN_RCV 。在这个状态下，Linux 内核就会建⽴⼀个「半连接队列」来维护「未完成」的握⼿信息，当半连接队列溢出后，服务端就⽆法再建⽴新的连接。

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211214170725.png)

SYN 攻击，攻击的是就是这个半连接队列。



### 3.90 如何查看由于 SYN 半连接队列已满，⽽被丢弃连接的情况？
我们可以通过该 netstat -s 命令给出的统计结果中，可以得到由于半连接队列已满，引发的失败次数：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211214172127.png)

上⾯输出的数值是**累计值**，表示共有多少个 TCP 连接因为半连接队列溢出⽽被丢弃。**隔⼏秒执⾏⼏次，如果有上升的趋势，说明当前存在半连接队列溢出的现象。**


### 3.91 如何调整 SYN 半连接队列⼤⼩？

要想增⼤半连接队列，不能只单纯增⼤ tcp_max_syn_backlog 的值，还需⼀同增⼤ somaxconn 和backlog，也就是增⼤ accept 队列。否则，只单纯增⼤ tcp_max_syn_backlog 是⽆效的。

增⼤ tcp_max_syn_backlog 和 somaxconn 的⽅法是修改 Linux 内核参数：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211214175510.png)


增⼤ backlog 的⽅式，每个 Web 服务都不同，⽐如 Nginx 增⼤ backlog 的⽅法如下：


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211214180037.png)



最后，改变了如上这些参数后，要重启 Nginx 服务，因为 SYN 半连接队列和 accept 队列都是在 listen() 初始化的。

### 3.92 如果 SYN 半连接队列已满，只能丢弃连接吗？

并不是这样，开启 syncookies 功能就可以在不使⽤ SYN 半连接队列的情况下成功建⽴连接。

syncookies 的⼯作原理：服务器根据当前状态计算出⼀个值，放在⼰⽅发出的 SYN+ACK 报⽂中发出，当客户端返回 ACK 报⽂时，取出该值验证，如果合法，就认为连接建⽴成功，如下图所示。

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211214180422.png)


syncookies 参数主要有以下三个值：

- 0 值，表示关闭该功能；

- 1 值，表示仅当 SYN 半连接队列放不下时，再启⽤它；

- 2 值，表示⽆条件开启功能；

那么在应对 SYN 攻击时，只需要设置为 1 即可：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211214180808.png)

### 3.93 SYN_RCV 状态的优化

当客户端接收到服务器发来的 SYN+ACK 报⽂后，就会回复 ACK 给服务器，同时客户端连接状态从 SYN_SENT
转换为 ESTABLISHED，表示连接建⽴成功。

服务器端连接成功建⽴的时间还要再往后，等到服务端收到客户端的 ACK 后，服务端的连接状态才变为
ESTABLISHED。


如果服务器没有收到 ACK，就会重发 SYN+ACK 报⽂，同时⼀直处于 SYN_RCV 状态。

当⽹络繁忙、不稳定时，报⽂丢失就会变严重，此时应该调⼤重发次数。反之则可以调⼩重发次数。修改重发次数的⽅法是，调整 tcp_synack_retries 参数：


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211214181516.png)

tcp_synack_retries 的默认重试次数是 5 次，与客户端重传 SYN 类似，它的重传会经历 1、2、4、8、16 秒，最后⼀次重传后会继续等待 32 秒，如果服务端仍然没有收到 ACK，才会关闭连接，故共需要等待 63 秒。


服务器收到 ACK 后连接建⽴成功，此时，内核会把连接从半连接队列移除，然后创建新的完全的连接，并将其添加到 accept 队列，等待进程调⽤ accept 函数时把连接取出来。

如果进程不能及时地调⽤ accept 函数，就会造成 accept 队列（也称全连接队列）溢出，最终导致建⽴好的 TCP连接被丢弃。

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211214182313.png)



### 3.94 accept 队列已满，只能丢弃连接吗？
丢弃连接只是 Linux 的默认⾏为，我们还可以选择向客户端发送 RST 复位报⽂，告诉客户端连接已经建⽴失败。打开这⼀功能需要将 tcp_abort_on_overflow 参数设置为 1。

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211214182659.png)

tcp_abort_on_overflow 共有两个值分别是 0 和 1，其分别表示：

- 0 ：如果 accept 队列满了，那么 server 扔掉 client 发过来的 ack ；

- 1 ：如果 accept 队列满了，server 发送⼀个 RST 包给 client，表示废掉这个握⼿过程和这个连接；


如果要想知道客户端连接不上服务端，是不是服务端 TCP 全连接队列满的原因，那么可以把tcp_abort_on_overflow 设置为 1，这时如果在客户端异常中可以看到很多 connection reset by peer 的错误，那么就可以证明是由于服务端 TCP 全连接队列溢出的问题。

通常情况下，应当把 tcp_abort_on_overflow 设置为 0，因为这样更有利于应对突发流量。

举个例⼦，当 accept 队列满导致服务器丢掉了 ACK，与此同时，客户端的连接状态却是 ESTABLISHED，客户端进程就在建⽴好的连接上发送请求。只要服务器没有为请求回复 ACK，客户端的请求就会被多次「重发」。**如果服务器上的进程只是短暂的繁忙造成 accept 队列满，那么当 accept 队列有空位时，再次接收到的请求报⽂由于含有 ACK，仍然会触发服务器端成功建⽴连接。**


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211214183739.png)


所以，tcp_abort_on_overflow 设为 0 可以提⾼连接建⽴的成功率，只有你⾮常肯定 TCP 全连接队列会⻓期溢出时，才能设置为 1 以尽快通知客户端。



### 3.95 如何调整 accept 队列的⻓度呢？

accept 队列的⻓度取决于 somaxconn 和 backlog 之间的最⼩值，也就是 min(somaxconn, backlog)，其中：

- somaxconn 是 Linux 内核的参数，默认值是 128，可以通过 net.core.somaxconn 来设置其值；

- backlog 是 listen(int sockfd, int backlog) 函数中的 backlog ⼤⼩；



Tomcat、Nginx、Apache 常⻅的 Web 服务的 backlog 默认值都是 511。


### 3.96 如何查看服务端进程 accept 队列的⻓度？

可以通过 ss -ltn 命令查看：


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211214194436.png)


- Recv-Q：当前 accept 队列的⼤⼩，也就是当前已完成三次握⼿并等待服务端 accept() 的 TCP 连接；

- Send-Q：accept 队列最⼤⻓度，上⾯的输出结果说明监听 8088 端⼝的 TCP 服务，accept 队列的最⼤⻓度
为 128；


### 3.97 如何查看由于 accept 连接队列已满，⽽被丢弃的连接？
当超过了 accept 连接队列，服务端则会丢掉后续进来的 TCP 连接，丢掉的 TCP 连接的个数会被统计起来，我们
可以使⽤ netstat -s 命令来查看：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211214195145.png)


上⾯看到的 41150 times ，表示 accept 队列溢出的次数，注意这个是累计值。可以隔⼏秒钟执⾏下，如果这个数字⼀直在增加的话，说明 accept 连接队列偶尔满了。

如果持续不断地有连接因为 accept 队列溢出被丢弃，就应该调⼤ backlog 以及 somaxconn 参数。



### 3.98 如何绕过三次握⼿？

以上我们只是在对三次握⼿的过程进⾏优化，接下来我们看看如何绕过三次握⼿发送数据。

三次握⼿建⽴连接造成的后果就是，HTTP 请求必须在⼀个 RTT（从客户端到服务器⼀个往返的时间）后才能发
送。


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211214195726.png)

在 Linux 3.7 内核版本之后，提供了 TCP Fast Open 功能，这个功能可以减少 TCP 连接建⽴的时延。

### 3.99 TCP Fast Open 功能的⼯作⽅式。
![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211214195912.png)

在客户端⾸次建⽴连接时的过程：

1. 客户端发送 SYN 报⽂，该报⽂包含 Fast Open 选项，且该选项的 Cookie 为空，这表明客户端请求 Fast
Open Cookie；

2. ⽀持 TCP Fast Open 的服务器⽣成 Cookie，并将其置于 SYN-ACK 数据包中的 Fast Open 选项以发回客户
端；

3. 客户端收到 SYN-ACK 后，本地缓存 Fast Open 选项中的 Cookie。

所以，第⼀次发起 HTTP GET 请求的时候，还是需要正常的三次握⼿流程。

之后，如果客户端再次向服务器建⽴连接时的过程：

1. 客户端发送 SYN 报⽂，该报⽂包含「数据」（对于⾮ TFO 的普通 TCP 握⼿过程，SYN 报⽂中不包含「数
据」）以及此前记录的 Cookie；

2. ⽀持 TCP Fast Open 的服务器会对收到 Cookie 进⾏校验：如果 Cookie 有效，服务器将在 SYN-ACK 报⽂中
对 SYN 和「数据」进⾏确认，服务器随后将「数据」递送⾄相应的应⽤程序；如果 Cookie ⽆效，服务器将
丢弃 SYN 报⽂中包含的「数据」，且其随后发出的 SYN-ACK 报⽂将只确认 SYN 的对应序列号；


3. 如果服务器接受了 SYN 报⽂中的「数据」，服务器可在握⼿完成之前发送「数据」，这就减少了握⼿带来的
1 个 RTT 的时间消耗；

4. 客户端将发送 ACK 确认服务器发回的 SYN 以及「数据」，但如果客户端在初始的 SYN 报⽂中发送的「数
据」没有被确认，则客户端将重新发送「数据」；

5. 此后的 TCP 连接的数据传输过程和⾮ TFO 的正常情况⼀致。

所以，之后发起 HTTP GET 请求的时候，可以绕过三次握⼿，这就减少了握⼿带来的 1 个 RTT 的时间消耗。
开启了 TFO 功能，cookie 的值是存放到 TCP option 字段⾥的：


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211214200445.png)


注：客户端在请求并存储了 Fast Open Cookie 之后，可以不断 复 TCP Fast Open 直⾄服务器认为 Cookie ⽆效（通常为过期）。



### 3.100 Linux 下怎么打开 TCP Fast Open 功能呢？

在 Linux 系统中，可以通过设置 tcp_fastopn 内核参数，来打开 Fast Open 功能：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211214200941.png)

tcp_fastopn 各个值的意义:

- 0 关闭

- 1 作为客户端使⽤ Fast Open 功能

- 2 作为服务端使⽤ Fast Open 功能

- 3 ⽆论作为客户端还是服务器，都可以使⽤ Fast Open 功能

TCP Fast Open 功能需要客户端和服务端同时⽀持，才有效果。


### 3.101 优化策略总结

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211214201249.png)


#### 客户端的优化
当客户端发起 SYN 包时，可以通过 tcp_syn_retries 控制其重传的次数。

#### 服务端的优化
当服务端 SYN 半连接队列溢出后，会导致后续连接被丢弃，可以通过 netstat -s 观察半连接队列溢出的情况，如果 SYN 半连接队列溢出情况⽐较严重，可以通过 tcp_max_syn_backlog、somaxconn、backlog 参数来调整
SYN 半连接队列的⼤⼩。


服务端回复 SYN+ACK 的 传次数由 tcp_synack_retries 参数控制。如果遭受 SYN 攻击，应把tcp_syncookies 参数设置为 1，表示仅在 SYN 队列满后开启 syncookie 功能，可以保证正常的连接成功建⽴。

服务端收到客户端返回的 ACK，会把连接移⼊ accpet 队列，等待进⾏调⽤ accpet() 函数取出连接。

可以通过 ss -lnt 查看服务端进程的 accept 队列⻓度，如果 accept 队列溢出，系统默认丢弃 ACK，如果可以把tcp_abort_on_overflow 设置为 1 ，表示⽤ RST 通知客户端连接建⽴失败。


如果 accpet 队列溢出严重，可以通过 listen 函数的 backlog 参数和 somaxconn 系统参数提⾼队列⼤⼩，
accept 队列⻓度取决于 min(backlog, somaxconn)。



#### 绕过三次握⼿
TCP Fast Open 功能可以绕过三次握⼿，使得 HTTP 请求减少了 1 个 RTT 的时间，Linux 下可以通过
tcp_fastopen 开启该功能，同时必须保证服务端和客户端同时⽀持。



### 3.102 TCP 四次挥⼿的性能提升

在开始之前，我们得先了解四次挥⼿状态变迁的过程。

客户端和服务端双⽅都可以主动断开连接，**通常先关闭连接的⼀⽅称为主动⽅，后关闭连接的⼀⽅称为被动⽅。**

可以看到，四次挥⼿过程只涉及了两种报⽂，分别是 FIN 和 ACK：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211214202037.png)


- FIN 就是结束连接的意思，谁发出 FIN 报⽂，就表示它将不会再发送任何数据，关闭这⼀⽅向上的传输通
道；


- ACK 就是确认的意思，⽤来通知对⽅：你⽅的发送通道已经关闭；



#### 四次挥⼿的过程:
- 当主动⽅关闭连接时，会发送 FIN 报⽂，此时发送⽅的 TCP 连接将从 ESTABLISHED 变成 FIN_WAIT1。

- 当被动⽅收到 FIN 报⽂后，内核会⾃动回复 ACK 报⽂，连接状态将从 ESTABLISHED 变成 CLOSE_WAIT，
表示被动⽅在等待进程调⽤ close 函数关闭连接。

- 当主动⽅收到这个 ACK 后，连接状态由 FIN_WAIT1 变为 FIN_WAIT2，也就是表示 **主动⽅的发送通道就关闭
了.**

- 当被动⽅进⼊ CLOSE_WAIT 时，被动⽅还会继续处理数据，等到进程的 read 函数返回 0 后，应⽤程序就会
调⽤ close 函数，进⽽触发内核发送 FIN 报⽂，此时被动⽅的连接状态变为 LAST_ACK。


- 当主动⽅收到这个 FIN 报⽂后，内核会回复 ACK 报⽂给被动⽅，同时主动⽅的连接状态由 FIN_WAIT2 变为
TIME_WAIT，在 Linux 系统下⼤约等待 1 分钟后，TIME_WAIT 状态的连接才会彻底关闭。

- 当被动⽅收到最后的 ACK 报⽂后，被动⽅的连接就会关闭。


当被动⽅收到最后的 ACK 报⽂后，被动⽅的连接就会关闭。


这⾥⼀点需要注意是：主动关闭连接的，才有 TIME_WAIT 状态。

主动关闭⽅和被动关闭⽅优化的思路也不同，接下来分别说说如何优化他们。


### 3.103 主动⽅的优化
关闭连接的⽅式通常有两种，分别是 RST 报⽂关闭和 FIN 报⽂关闭。

如果进程异常退出了，内核就会发送 RST 报⽂来关闭，它可以不⾛四次挥⼿流程，是⼀个暴⼒关闭连接的⽅式。

安全关闭连接的⽅式必须通过四次挥⼿，它由进程调⽤ close 和 shutdown 函数发起 FIN 报⽂（shutdown 参
数须传⼊ SHUT_WR 或者 SHUT_RDWR 才会发送 FIN）。

#### 调⽤ close 函数和 shutdown 函数有什么区别？

调⽤了 close 函数意味着完全断开连接，完全断开不仅指⽆法传输数据，⽽且也不能发送数据。 此时，调⽤了
close 函数的⼀⽅的连接叫做「孤⼉连接」，如果你⽤ netstat -p 命令，会发现连接对应的进程名为空。

使⽤ close 函数关闭连接是不优雅的。于是，就出现了⼀种优雅关闭连接的 shutdown 函数，它可以控制只关闭
⼀个⽅向的连接：



![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211215094041.png)


第⼆个参数决定断开连接的⽅式，主要有以下三种⽅式：

- SHUT_RD(0)：关闭连接的「读」这个⽅向，如果接收缓冲区有已接收的数据，则将会被丢弃，并且后续再收
到新的数据，会对数据进⾏ ACK，然后悄悄地丢弃。也就是说，对端还是会接收到 ACK，在这种情况下根本
不知道数据已经被丢弃了。


- SHUT_WR(1)：关闭连接的「写」这个⽅向，这就是常被称为「半关闭」的连接。如果发送缓冲区还有未发
送的数据，将被⽴即发送出去，并发送⼀个 FIN 报⽂给对端。


- SHUT_RDWR(2)：相当于 SHUT_RD 和 SHUT_WR 操作各⼀次，关闭套接字的读和写两个⽅向。

close 和 shutdown 函数都可以关闭连接，但这两种⽅式关闭的连接，不只功能上有差异，控制它们的 Linux 参数也不相同。



#### FIN_WAIT1 状态的优化

主动⽅发送 FIN 报⽂后，连接就处于 FIN_WAIT1 状态，正常情况下，如果能及时收到被动⽅的 ACK，则会很快变
为 FIN_WAIT2 状态。

但是当迟迟收不到对⽅返回的 ACK 时，连接就会⼀直处于 FIN_WAIT1 状态。此时，内核会定时重发 FIN 报⽂，
其中重发次数由 tcp_orphan_retries 参数控制（注意，orphan 虽然是孤⼉的意思，该参数却不只对孤⼉连接有
效，事实上，它对所有 FIN_WAIT1 状态下的连接都有效），默认值是 0。

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211215094946.png)

你可能会好奇，这 0 表示⼏次？实际上当为 0 时，特指 8 次

如果 FIN_WAIT1 状态连接很多，我们就需要考虑降低 tcp_orphan_retries 的值，当重传次数超过
tcp_orphan_retries 时，连接就会直接关闭掉

对于普遍正常情况时，调低 tcp_orphan_retries 就已经可以了。如果遇到恶意攻击，FIN 报⽂根本⽆法发送出去，这由 TCP 两个特性导致的：

- ⾸先，TCP 必须保证报⽂是有序发送的，FIN 报⽂也不例外，当发送缓冲区还有数据没有发送时，FIN 报⽂
也不能提前发送。

- 其次，TCP 有流量控制功能，当接收⽅接收窗⼝为 0 时，发送⽅就不能再发送数据。所以，当攻击者下载⼤
⽂件时，就可以通过接收窗⼝设为 0 ，这就会使得 FIN 报⽂都⽆法发送出去，那么连接会⼀直处于
FIN_WAIT1 状态。

解决这种问题的⽅法，是调整 tcp_max_orphans 参数，它定义了「孤⼉连接」的最⼤数量：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211215101233.png)

当进程调⽤了 close 函数关闭连接，此时连接就会是「孤⼉连接」，因为它⽆法再发送和接收数据。Linux 系统
为了防⽌孤⼉连接过多，导致系统资源⻓时间被占⽤，就提供了 tcp_max_orphans 参数。如果孤⼉连接数量⼤
于它，新增的孤⼉连接将不再⾛四次挥⼿，⽽是直接发送 RST 复位报⽂强制关闭。


#### FIN_WAIT2 状态的优化
当主动⽅收到 ACK 报⽂后，会处于 FIN_WAIT2 状态，就表示主动⽅的发送通道已经关闭，接下来将等待对⽅发送
FIN 报⽂，关闭对⽅的发送通道。

这时，如果连接是⽤ shutdown 函数关闭的，连接可以⼀直处于 FIN_WAIT2 状态，因为它可能还可以发送或接收
数据。但对于 close 函数关闭的孤⼉连接，由于⽆法再发送和接收数据，所以这个状态不可以持续太久，⽽
tcp_fin_timeout 控制了这个状态下连接的持续时⻓，默认值是 60 秒：


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211215102114.png)


它意味着对于孤⼉连接（调⽤ close 关闭的连接），如果在 60 秒后还没有收到 FIN 报⽂，连接就会直接关闭。

这个 60 秒不是随便决定的，它与 TIME_WAIT 状态持续的时间是相同的，后⾯我们再来说说为什么是 60 秒。


#### TIME_WAIT 状态的优化
TIME_WAIT 是主动⽅四次挥⼿的最后⼀个状态，也是最常遇⻅的状态。

当收到被动⽅发来的 FIN 报⽂后，主动⽅会⽴刻回复 ACK，表示确认对⽅的发送通道已经关闭，接着就处于
TIME_WAIT 状态。在 Linux 系统，TIME_WAIT 状态会持续 60 秒后才会进⼊关闭状态。

TIME_WAIT 状态的连接，在主动⽅看来确实快已经关闭了。然后，被动⽅没有收到 ACK 报⽂前，还是处于
LAST_ACK 状态。如果这个 ACK 报⽂没有到达被动⽅，被动⽅就会重发 FIN 报⽂。 重发次数仍然由前⾯介绍过的
tcp_orphan_retries 参数控制。

TIME-WAIT 的状态尤其重要，主要是两个原因：
- 防⽌具有相同「四元组」的「旧」数据包被收到；

- 保证「被动关闭连接」的⼀⽅能被正确的关闭，即保证最后的 ACK 能让被动关闭⽅接收，从⽽帮助其正常关闭


原因⼀：防⽌旧连接的数据包

TIME-WAIT 的⼀个作⽤是防⽌收到历史数据，从⽽导致数据错乱的问题。

假设 TIME-WAIT 没有等待时间或时间过短，被延迟的数据包抵达后会发⽣什么呢？

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211215104138.png)

- 如上图⻩⾊框框服务端在关闭连接之前发送的 SEQ = 301 报⽂，被⽹络延迟了。
- 这时有相同端⼝的 TCP 连接被复⽤后，被延迟的 SEQ = 301 抵达了客户端，那么客户端是有可能正常接收
这个过期的报⽂，这就会产⽣数据错乱等严重的问题。


所以，TCP 就设计出了这么⼀个机制，经过 2MSL 这个时间，⾜以让两个⽅向上的数据包都被丢弃，使得原来
连接的数据包在⽹络中都⾃然消失，再出现的数据包⼀定都是新建⽴连接所产⽣的。


原因⼆：保证连接正确关闭

TIME-WAIT 的另外⼀个作⽤是等待⾜够的时间以确保最后的 ACK 能让被动关闭⽅接收，从⽽帮助其正常关闭。

假设 TIME-WAIT 没有等待时间或时间过短，断开连接会造成什么问题呢？

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211215105146.png)

- 如上图红⾊框框客户端四次挥⼿的最后⼀个 ACK 报⽂如果在⽹络中被丢失了，此时如果客户端 TIME-
WAIT 过短或没有，则就直接进⼊了 CLOSE 状态了，那么服务端则会⼀直处在 LAST-ACK 状态。

- 当客户端发起建⽴连接的 SYN 请求报⽂后，服务端会发送 RST 报⽂给客户端，连接建⽴的过程就会被
终⽌。


我们再回过头来看看，为什么 TIME_WAIT 状态要保持 60 秒呢？这与孤⼉连接 FIN_WAIT2 状态默认保留 60 秒的原理是⼀样的，因为这两个状态都需要保持 2MSL 时⻓。MSL 全称是 Maximum Segment Lifetime，它定义了
⼀个报⽂在⽹络中的最⻓⽣存时间（报⽂每经过⼀次路由器的转发，IP 头部的 TTL 字段就会减 1，减到 0 时报⽂就被丢弃，这就限制了报⽂的最⻓存活时间）。


为什么是 2 MSL 的时⻓呢？这其实是相当于⾄少允许报⽂丢失⼀次。⽐如，若 ACK 在⼀个 MSL 内丢失，这样被
动⽅ 发的 FIN 会在第 2 个 MSL 内到达，TIME_WAIT 状态的连接可以应对。

为什么不是 4 或者 8 MSL 的时⻓呢？你可以想象⼀个丢包率达到百分之⼀的糟糕⽹络，连续两次丢包的概率只有
万分之⼀，这个概率实在是太⼩了，忽略它⽐解决它更具性价⽐。

因此，TIME_WAIT 和 FIN_WAIT2 状态的最⼤时⻓都是 2 MSL，由于在 Linux 系统中，MSL 的值固定为 30 秒，
所以它们都是 60 秒。


虽然 TIME_WAIT 状态有存在的必要，但它毕竟会消耗系统资源。如果发起连接⼀⽅的 TIME_WAIT 状态过多，占
满了所有端⼝资源，则会导致⽆法创建新连接。


- 客户端受端⼝资源限制：如果客户端 TIME_WAIT 过多，就会导致端⼝资源被占⽤，因为端⼝就65536个，被
占满就会导致⽆法创建新的连接；

- 服务端受系统资源限制：由于⼀个四元组表示TCP连接，理论上服务端可以建⽴很多连接，服务端确实只监
听⼀个端⼝，但是会把连接扔给处理线程，所以理论上监听的端⼝可以继续监听。但是线程池处理不了那么
多⼀直不断的连接了。所以当服务端出现⼤量TIME_WAIT 时，系统资源被占满时，会导致处理不过来新的连
接；


另外，Linux 提供了 tcp_max_tw_buckets 参数，当 TIME_WAIT 的连接数量超过该参数时，新关闭的连接就不
再经历 TIME_WAIT ⽽直接关闭：


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211215110722.png)

当服务器的并发连接增多时，相应地，同时处于 TIME_WAIT 状态的连接数量也会变多，此时就应当调⼤
tcp_max_tw_buckets 参数，减少不同连接间数据错乱的概率。

cp_max_tw_buckets 也不是越⼤越好，毕竟内存和端⼝都是有限的。


有⼀种⽅式可以在建⽴新连接时，复⽤处于 TIME_WAIT 状态的连接，那就是打开 tcp_tw_reuse 参数。但是需要
注意，该参数是只⽤于客户端（建⽴连接的发起⽅），因为是在调⽤ connect() 时起作⽤的，⽽对于服务端（被动连接⽅）是没有⽤的。

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211215112409.png)

tcp_tw_reuse 从协议⻆度理解是安全可控的，可以复⽤处于 TIME_WAIT 的端⼝为新的连接所⽤。

什么是协议⻆度理解的安全可控呢？主要有两点：

- 只适⽤于连接发起⽅，也就是 C/S 模型中的客户端；

- 对应的 TIME_WAIT 状态的连接创建时间超过 1 秒才可以被复⽤。

使⽤这个选项，还有⼀个前提，需要打开对 TCP 时间戳的⽀持（对⽅也要打开 ）：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211215113136.png)

由于引⼊了时间戳，它能带来了些好处：

- 我们在前⾯提到的 2MSL 问题就不复存在了，因为重复的数据包会因为时间戳过期被⾃然丢弃；

- 同时，它还可以防⽌序列号绕回，也是因为重复的数据包会由于时间戳过期被⾃然丢弃；

时间戳是在 TCP 的选项字段⾥定义的，开启了时间戳功能，在 TCP 报⽂传输的时候会带上发送报⽂的时间戳。


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211215113838.png)


我们来看看开启了 tcp_tw_reuse 功能，如果四次挥⼿中的最后⼀次 ACK 在⽹络中丢失了，会发⽣什么？


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211215114053.png)


上图的流程：


- 四次挥⼿中的最后⼀次 ACK 在⽹络中丢失了，服务端⼀直处于 LAST_ACK 状态；

- 客户端由于开启了 tcp_tw_reuse 功能，客户端再次发起新连接的时候，会复⽤超过 1 秒后的 time_wait 状态的连接。但客户端新发的 SYN 包会被忽略（由于时间戳），因为服务端⽐较了客户端的上⼀个报⽂与 SYN
报⽂的时间戳，过期的报⽂就会被服务端丢弃；

- 服务端 FIN 报⽂迟迟没有收到四次挥⼿的最后⼀次 ACK，于是超时重发了 FIN 报⽂给客户端；


- 处于 SYN_SENT 状态的客户端，由于收到了 FIN 报⽂，则会回 RST 给服务端，于是服务端就离开了
LAST_ACK 状态；

- 最初的客户端 SYN 报⽂超时重发了（ 1 秒钟后），此时就与服务端能正确的三次握⼿了。


所以⼤家都会说开启了 tcp_tw_reuse，可以在复⽤了 time_wait 状态的 1 秒过后成功建⽴连接，这 1 秒主要是花费在 SYN 包重传。

另外，⽼版本的 Linux 还提供了 tcp_tw_recycle 参数，但是当开启了它，就有两个坑：


- Linux 会加快客户端和服务端 TIME_WAIT 状态的时间，也就是它会使得 TIME_WAIT 状态会⼩于 60 秒，很
容易导致数据错乱；

- 另外，Linux 会丢弃所有来⾃远端时间戳⼩于上次记录的时间戳（由同⼀个远端发送的）的任何数据包。就
是说要使⽤该选项，则必须保证数据包的时间戳是单调递增的。那么，问题在于，此处的时间戳并不是我们
通常意义上⾯的绝对时间，⽽是⼀个相对时间。很多情况下，我们是没法保证时间戳单调递增的，⽐如使⽤
了 NAT、LVS 等情况；


所以，不建议设置为 1 ，在 Linux 4.12 版本后，Linux 内核直接取消了这⼀参数，建议关闭它：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211215121609.png)

另外，我们可以在程序中设置 socket 选项，来设置调⽤ close 关闭连接⾏为。


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211215121612.png)


如果 l_onoff 为⾮ 0， 且 l_linger 值为 0，那么调⽤ close 后，会⽴该发送⼀个 RST 标志给对端，该 TCP 连接将跳过四次挥⼿，也就跳过了 TIME_WAIT 状态，直接关闭。

但这为跨越 TIME_WAIT 状态提供了⼀个可能，不过是⼀个⾮常危险的⾏为，不值得提倡。


### 3.103 被动⽅的优化
当被动⽅收到 FIN 报⽂时，内核会⾃动回复 ACK，同时连接处于 CLOSE_WAIT 状态，顾名思义，它表示等待应⽤
进程调⽤ close 函数关闭连接。

内核没有权利替代进程去关闭连接，因为如果主动⽅是通过 shutdown 关闭连接，那么它就是想在半关闭连接上接
收数据或发送数据。因此，Linux 并没有限制 CLOSE_WAIT 状态的持续时间。

当然，⼤多数应⽤程序并不使⽤ shutdown 函数关闭连接。所以，当你⽤ netstat 命令发现⼤量 CLOSE_WAIT 状
态。就需要排查你的应⽤程序，因为可能因为应⽤程序出现了 Bug，read 函数返回 0 时，没有调⽤ close 函数。

处于 CLOSE_WAIT 状态时，调⽤了 close 函数，内核就会发出 FIN 报⽂关闭发送通道，同时连接进⼊ LAST_ACK
状态，等待主动⽅返回 ACK 来确认连接关闭。


如果迟迟收不到这个 ACK，内核就会 发 FIN 报⽂， 发次数仍然由 tcp_orphan_retries 参数控制，这与主动⽅
重发 FIN 报⽂的优化策略⼀致。


还有⼀点我们需要注意的，如果被动⽅迅速调⽤ close 函数，那么被动⽅的 ACK 和 FIN 有可能在⼀个报⽂中发
送，这样看起来，四次挥⼿会变成三次挥⼿，这只是⼀种特殊情况，不⽤在意。

#### 如果连接双⽅同时关闭连接，会怎么样？
由于 TCP 是双全⼯的协议，所以是会出现两⽅同时关闭连接的现象，也就是同时发送了 FIN 报⽂。

此时，上⾯介绍的优化策略仍然适⽤。两⽅发送 FIN 报⽂时，都认为⾃⼰是主动⽅，所以**都进⼊了 FIN_WAIT1 状态**，FIN 报⽂的重发次数仍由 tcp_orphan_retries 参数控制。

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211215122839.png)

接下来，双⽅在等待 ACK 报⽂的过程中，都等来了 FIN 报⽂。这是⼀种新情况，所以连接会进⼊⼀种叫做
CLOSING 的新状态，它替代了 FIN_WAIT2 状态。

接着，双⽅内核回复 ACK 确认对⽅发送通道的关闭后，进⼊TIME_WAIT 状态，等待 2MSL 的时间后，连接⾃动关闭。

### 3.104 小结 TCP 四次挥⼿的优化
我们需要根据主动⽅和被动⽅四次挥⼿状态变化来调整系统 TCP 内核参数。

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211215132654.png)

#### 主动⽅的优化
主动发起 FIN 报⽂断开连接的⼀⽅，如果迟迟没收到对⽅的 ACK 回复，则会 传 FIN 报⽂， 传的次数由
tcp_orphan_retries 参数决定。

当主动⽅收到 ACK 报⽂后，连接就进⼊ FIN_WAIT2 状态，根据关闭的⽅式不同，优化的⽅式也不同：

- 如果这是 close 函数关闭的连接，那么它就是孤⼉连接。如果 tcp_fin_timeout 秒内没有收到对⽅的 FIN 报
⽂，连接就直接关闭。同时，为了应对孤⼉连接占⽤太多的资源， tcp_max_orphans 定义了最⼤孤⼉连接
的数 ，超过时连接就会直接释放。


- 反之是 shutdown 函数关闭的连接，则不受此参数限制；

当主动⽅接收到 FIN 报⽂，并返回 ACK 后，主动⽅的连接进⼊ TIME_WAIT 状态。这⼀状态会持续 1 分钟，为了防⽌ TIME_WAIT 状态占⽤太多的资源， tcp_max_tw_buckets 定义了最⼤数量，超过时连接也会直接释放。


当 TIME_WAIT 状态过多时，还可以通过设置 tcp_tw_reuse 和 tcp_timestamps 为 1 ，将 TIME_WAIT 状态的
端⼝复⽤于作为客户端的新连接，注意该参数只适⽤于客户端。

#### 被动⽅的优化
被动关闭的连接⽅应对⾮常简单，它在回复 ACK 后就进⼊了 CLOSE_WAIT 状态，等待进程调⽤ close 函数关闭连接。因此，出现⼤量CLOSE_WAIT 状态的连接时，应当从应⽤程序中找问题。

当被动⽅发送 FIN 报⽂后，连接就进⼊ LAST_ACK 状态，在未等到 ACK 时，会在 tcp_orphan_retries 参数的控制下重发 FIN 报⽂。


### 3.105 TCP 传输数据的性能提升
TCP 连接是由内核维护的，内核会为每个连接建⽴内存缓冲区：

- 如果连接的内存配置过⼩，就⽆法充分使⽤⽹络带宽，TCP 传输效率就会降低；

- 如果连接的内存配置过⼤，很容易把服务器资源耗尽，这样就会导致新连接⽆法建⽴；

因此，我们必须理解 Linux 下 TCP 内存的⽤途，才能正确地配置内存⼤⼩。


#### 滑动窗⼝是如何影响传输速度的？

TCP 会保证每⼀个报⽂都能够抵达对⽅，它的机制是这样：报⽂发出去后，必须接收到对⽅返回的确认报⽂
ACK，如果迟迟未收到，就会超时重发该报⽂，直到收到对⽅的 ACK 为⽌。

所以，TCP 报⽂发出去后，并不会⽴⻢从内存中删除，因为重传时还需要⽤到它。

由于 TCP 是内核维护的，所以报⽂存放在内核缓冲区。如果连接⾮常多，我们可以通过 free 命令观察到
buff/cache 内存是会增⼤。


如果 TCP 是每发送⼀个数据，都要进⾏⼀次确认应答。当上⼀个数据包收到了应答了， 再发送下⼀个。这个模式
就有点像我和你⾯对⾯聊天，你⼀句我⼀句，但这种⽅式的缺点是效率⽐较低的。


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211216142741.png)



所以，这样的传输⽅式有⼀个缺点：数据包的往返时间越⻓，通信的效率就越低。


要解决这⼀问题不难，**并⾏批量发送报⽂，再批量确认报⽂即可。**

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211216143233.png)


然⽽，这引出了另⼀个问题，发送⽅可以随⼼所欲的发送报⽂吗？当然这不现实，我们还得考虑接收⽅的处理能
⼒。


当接收⽅硬件不如发送⽅，或者系统繁忙、资源紧张时，是⽆法瞬间处理这么多报⽂的。于是，这些报⽂只能被丢
掉，使得⽹络效率⾮常低。

为了解决这种现象发⽣，TCP 提供⼀种机制可以让「发送⽅」根据「接收⽅」的实际接收能⼒控制发送的数据量，
这就是滑动窗⼝的由来。


接收⽅根据它的缓冲区，可以计算出后续能够接收多少字节的报⽂，这个数字叫做接收窗⼝。当内核接收到报⽂
时，必须⽤缓冲区存放它们，这样剩余缓冲区空间变⼩，接收窗⼝也就变⼩了；当进程调⽤ read 函数后，数据被
读⼊了⽤户空间，内核缓冲区就被清空，这意味着主机可以接收更多的报⽂，接收窗⼝就会变⼤。

因此，接收窗⼝并不是恒定不变的，接收⽅会把当前可接收的⼤⼩放在 TCP 报⽂头部中的窗⼝字段，这样就可以
起到窗⼝⼤⼩通知的作⽤。


发送⽅的窗⼝等价于接收⽅的窗⼝吗？如果不考虑拥塞控制，发送⽅的窗⼝⼤⼩「约等于」接收⽅的窗⼝⼤⼩，因
为窗⼝通知报⽂在⽹络传输是存在时延的，所以是约等于的关系。

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211216144439.png)

从上图中可以看到，窗⼝字段只有 2 个字节，因此它最多能表达 65535 字节⼤⼩的窗⼝，也就是 64KB ⼤⼩。

这个窗⼝⼤⼩最⼤值，在当今⾼速⽹络下，很明显是不够⽤的。所以后续有了扩充窗⼝的⽅法：在 TCP 选项字段
定义了窗⼝扩⼤因⼦，⽤于扩⼤ TCP 通告窗⼝，其值⼤⼩是 2^14，这样就使 TCP 的窗⼝⼤⼩从 16 位扩⼤为 30
位（2^16 * 2^ 14 = 2^30），所以此时窗⼝的最⼤值可以达到 1GB。

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211216144740.png)



Linux 中打开这⼀功能，需要把 tcp_window_scaling 配置设为 1（默认打开）：


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211216144844.png)


要使⽤窗⼝扩⼤选项，通讯双⽅必须在各⾃的 SYN 报⽂中发送这个选项：

- 主动建⽴连接的⼀⽅在 SYN 报⽂中发送这个选项；

- ⽽被动建⽴连接的⼀⽅只有在收到带窗⼝扩⼤选项的 SYN 报⽂之后才能发送这个选项。


这样看来，只要进程能及时地调⽤ read 函数读取数据，并且接收缓冲区配置得⾜够⼤，那么接收窗⼝就可以⽆限
地放⼤，发送⽅也就⽆限地提升发送速度。

这是不可能的，因为⽹络的传输能⼒是有限的，当发送⽅依据发送窗⼝，发送超过⽹络处理能⼒的报⽂时，路由器
会直接丢弃这些报⽂。因此，缓冲区的内存并不是越⼤越好。

#### 如何确定最⼤传输速度？
在前⾯我们知道了 TCP 的传输速度，受制于发送窗⼝与接收窗⼝，以及⽹络设备传输能⼒。其中，窗⼝⼤⼩由内
核缓冲区⼤⼩决定。如果缓冲区与⽹络传输能⼒匹配，那么缓冲区的利⽤率就达到了最⼤化。


问题来了，如何计算⽹络的传输能⼒呢？

相信⼤家都知道⽹络是有「带宽」限制的，带宽描述的是⽹络传输能⼒，它与内核缓冲区的计量单位不同:

- 带宽是单位时间内的流量，表达是「速度」，⽐如常⻅的带宽 100 MB/s；

- 缓冲区单位是字节，当⽹络速度乘以时间才能得到字节数；

这⾥需要说⼀个概念，就是带宽时延积，它决定⽹络中⻜⾏报⽂的⼤⼩，它的计算⽅式：


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211216151232.png)


⽐如最⼤带宽是 100 MB/s，⽹络时延（RTT）是 10ms 时，意味着客户端到服务端的⽹络⼀共可以存放 100MB/s
* 0.01s = 1MB 的字节。


这个 1MB 是带宽和时延的乘积，所以它就叫「带宽时延积」（缩写为 BDP，Bandwidth Delay Product）。同
时，这 1MB 也表示「⻜⾏中」的 TCP 报⽂⼤⼩，它们就在⽹络线路、路由器等⽹络设备上。如果⻜⾏报⽂超过了
1 MB，就会导致⽹络过载，容易丢包。


**由于发送缓冲区⼤⼩决定了发送窗⼝的上限，⽽发送窗⼝⼜决定了「已发送未确认」的⻜⾏报⽂的上限。因此，发送缓冲区不能超过「带宽时延积」。**

发送缓冲区与带宽时延积的关系：


- 如果发送缓冲区「超过」带宽时延积，超出的部分就没办法有效的⽹络传输，同时导致⽹络过载，容易丢
包；

- 如果发送缓冲区「⼩于」带宽时延积，就不能很好的发挥出⽹络的传输效率。


所以，发送缓冲区的⼤⼩最好是往带宽时延积靠近。


#### 怎样调整缓冲区⼤⼩？

在 Linux 中发送缓冲区和接收缓冲都是可以⽤参数调节的。设置完后，Linux 会根据你设置的缓冲区进⾏动态调
节。

##### 调节发送缓冲区范围

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211216152503.png)

上⾯三个数字单位都是字节，它们分别表示：

- 第⼀个数值是动态范围的最⼩值，4096 byte = 4K；
- 第⼆个数值是初始默认值，87380 byte ≈ 86K；
- 第三个数值是动态范围的最⼤值，4194304 byte = 4096K（4M）；

发送缓冲区是⾃⾏调节的，当发送⽅发送的数据被确认后，并且没有新的数据要发送，就会把发送缓冲区的内存释
放掉。


##### 调节接收缓冲区范围

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211216152515.png)


上⾯三个数字单位都是字节，它们分别表示：


- 第⼀个数值是动态范围的最⼩值，表示即使在内存压⼒下也可以保证的最⼩接收缓冲区⼤⼩，4096 byte =
4K；

- 第⼆个数值是初始默认值，87380 byte ≈ 86K；

- 第三个数值是动态范围的最⼤值，6291456 byte = 6144K（6M）；



**接收缓冲区可以根据系统空闲内存的⼤⼩来调节接收窗⼝：**

- 如果系统的空闲内存很多，就可以⾃动把缓冲区增⼤⼀些，这样传给对⽅的接收窗⼝也会变⼤，因⽽提升发
送⽅发送的传输数据数量；

- 反之，如果系统的内存很紧张，就会减少缓冲区，这虽然会降低传输效率，可以保证更多的并发连接正常⼯
作；


发送缓冲区的调节功能是⾃动开启的，⽽接收缓冲区则需要配置 tcp_moderate_rcvbuf 为 1 来开启调节功能


![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211216153948.png)

##### 调节 TCP 内存范围
接收缓冲区调节时，怎么知道当前内存是否紧张或充分呢？这是通过 tcp_mem 配置完成的：

![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211216154147.png)

上⾯三个数字单位不是字节，⽽是「⻚⾯⼤⼩」，1 ⻚表示 4KB，它们分别表示：


- 当 TCP 内存⼩于第 1 个值时，不需要进⾏⾃动调节；

- 在第 1 和第 2 个值之间时，内核开始调节接收缓冲区的⼤⼩；

- ⼤于第 3 个值时，内核不再为 TCP 分配新内存，此时新连接是⽆法建⽴的；


⼀般情况下这些值是在系统启动时根据系统内存数量计算得到的。根据当前 tcp_mem 最⼤内存⻚⾯数是177120，当内存为 (177120 * 4) / 1024K ≈ 692M 时，系统将⽆法为新的 TCP 连接分配内存，即 TCP 连接将被拒绝。



##### 根据实际场景调节的策略

在⾼并发服务器中，为了兼顾⽹速与⼤ 的并发连接，我们应当**保证缓冲区的动态调整的最⼤值达到带宽时延积，⽽最⼩值保持默认的 4K 不变即可。⽽对于内存紧张的服务⽽⾔，调低默认值是提⾼并发的有效⼿段。**


同时，如果这是⽹络 IO 型服务器，那么，调⼤ tcp_mem 的上限可以让 TCP 连接使⽤更多的系统内存，这有利于
提升并发能⼒。需要注意的是，tcp_wmem 和 tcp_rmem 的单位是字节，⽽ tcp_mem 的单位是⻚⾯⼤⼩。⽽且，
千万不要在 socket 上直接设置 SO_SNDBUF 或者 SO_RCVBUF，这样会关闭缓冲区的动态调整功能。




### 3.106 小结TCP优化数据传输的方式
![](https://cdn.jsdelivr.net/gh/Gpslypy/mediaImage01@master/img202111/QQ图片20211216160426.png)

TCP 可靠性是通过 ACK 确认报⽂实现的，⼜依赖滑动窗⼝提升了发送速度也兼顾了接收⽅的处理能⼒。

可是，默认的滑动窗⼝最⼤值只有 64 KB，不满⾜当今的⾼速⽹络的要求，要想提升发送速度必须提升滑动窗⼝的
上限，在 Linux 下是通过设置 tcp_window_scaling 为 1 做到的，此时最⼤值可⾼达 1GB。


滑动窗⼝定义了⽹络中⻜⾏报⽂的最⼤字节数，当它超过带宽时延积时，⽹络过载，就会发⽣丢包。⽽当它⼩于带
宽时延积时，就⽆法充分利⽤⽹络带宽。因此，滑动窗⼝的设置，必须参考带宽时延积。

内核缓冲区决定了滑动窗⼝的上限，缓冲区可分为：发送缓冲区 tcp_wmem 和接收缓冲区 tcp_rmem。


Linux 会对缓冲区动态调节，我们应该把缓冲区的上限设置为带宽时延积。发送缓冲区的调节功能是⾃动打开的，
⽽接收缓冲区需要把 tcp_moderate_rcvbuf 设置为 1 来开启。其中，调节的依据是 TCP 内存范围 tcp_mem。


但需要注意的是，如果程序中的 socket 设置 SO_SNDBUF 和 SO_RCVBUF，则会关闭缓冲区的动态整功能，所以
不建议在程序设置它俩，⽽是交给内核⾃动调整⽐较好。

有效配置这些参数后，既能够最⼤程度地保持并发性，也能让资源充裕时连接传输速度达到最⼤值。

### 3.107 如果 accept 队列满了，那么 server 扔掉 client 发过来的 ack”，也就是说该TCP连接还是位于半连接队列中，没有丢弃吗？”

1. 当 accept 队列满了，后续新进来的syn包都会被丢失

2. 我⽂章的突发流量例⼦是，那个连接进来的时候 accept 队列还没满，但是在第三次握⼿的时候，accept 队
列突然满了，就会导致 ack 被丢弃，就⼀直处于半连接队列。